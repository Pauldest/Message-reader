"""RSS AI Reader - ä¸»ç¨‹åºå…¥å£ (Multi-Agent Version)"""

import argparse
import asyncio
import signal
import sys
from datetime import datetime
from pathlib import Path

# ç¡®ä¿å¯ä»¥æ‰¾åˆ° src åŒ…ï¼ˆæ”¯æŒç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼‰
if __name__ == "__main__":
    sys.path.insert(0, str(Path(__file__).parent.parent))

import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ]
)

try:
    # ä½œä¸ºæ¨¡å—è¿è¡Œ: python -m src.main
    from .config import get_config, reload_config, AppConfig
    from .fetcher import RSSParser, ContentExtractor
    from .agents import AnalysisOrchestrator
    from .models.agent import AnalysisMode
    from .models.article import Article as NewArticle, EnrichedArticle
    from .storage import Database, DigestArticle, DailyDigest, InformationStore
    from .storage.models import Article as LegacyArticle, AnalyzedArticle
    from .notifier import EmailSender
    from .scheduler import Scheduler
    from .services.telemetry import AITelemetry, get_telemetry
except ImportError:
    # ç›´æ¥è¿è¡Œ: python src/main.py
    from src.config import get_config, reload_config, AppConfig
    from src.fetcher import RSSParser, ContentExtractor
    from src.agents import AnalysisOrchestrator
    from src.models.agent import AnalysisMode
    from src.models.article import Article as NewArticle, EnrichedArticle
    from src.storage import Database, DigestArticle, DailyDigest, InformationStore
    from src.storage.models import Article as LegacyArticle, AnalyzedArticle
    from src.notifier import EmailSender
    from src.scheduler import Scheduler
    from src.services.telemetry import AITelemetry, get_telemetry

logger = structlog.get_logger()


class RSSReaderService:
    """RSS é˜…è¯»å™¨æœåŠ¡ (Multi-Agent Version)"""
    
    def __init__(self, config: AppConfig, analysis_mode: str = "deep", concurrency: int = 5):
        self.config = config
        self.concurrency = concurrency  # å¹¶å‘å¤„ç†æ•°é‡
        
        # è§£æåˆ†ææ¨¡å¼
        self.analysis_mode = AnalysisMode(analysis_mode)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db = Database(config.storage.database_path)
        self.rss_parser = RSSParser()
        self.content_extractor = ContentExtractor()
        
        # ğŸ†• å¤šæ™ºèƒ½ä½“åˆ†æå™¨
        self.orchestrator = AnalysisOrchestrator(config)
        
        self.email_sender = EmailSender(config.email)
        self.scheduler = Scheduler(config.schedule)
        
        # ğŸ†• åˆå§‹åŒ–é¥æµ‹æœåŠ¡
        AITelemetry.initialize(
            enabled=config.telemetry.enabled,
            storage_path=config.telemetry.storage_path,
            retention_days=config.telemetry.retention_days,
            max_content_length=config.telemetry.max_content_length,
        )
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
        
        logger.info(
            "service_initialized",
            analysis_mode=self.analysis_mode.value,
            vector_store=self.orchestrator.get_stats().get("vector_store", {}),
        )
        
        # åˆå§‹åŒ–ä¿¡æ¯å­˜å‚¨å¹¶æ³¨å…¥åè°ƒå™¨ï¼ˆä¼ å…¥å‘é‡å­˜å‚¨ä»¥å¯ç”¨è¯­ä¹‰å»é‡ï¼‰
        self.info_store = InformationStore(self.db, vector_store=self.orchestrator.vector_store)
        self.orchestrator.set_information_store(self.info_store)
        
        # ğŸ†• åˆå§‹åŒ–å®ä½“å­˜å‚¨ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰
        from .storage.entity_store import EntityStore
        self.entity_store = EntityStore(self.db)
        self.orchestrator.set_entity_store(self.entity_store)
    
    async def fetch_and_analyze(self, limit: int = None):
        """æŠ“å–å¹¶åˆ†ææ–‡ç« 
        
        Args:
            limit: é™åˆ¶åˆ†æçš„æ–‡ç« æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        logger.info("starting_fetch_cycle", mode=self.analysis_mode.value, limit=limit)
        
        try:
            # 1. æŠ“å– RSS
            articles = await self.rss_parser.fetch_all(self.config.feeds)
            
            if not articles:
                logger.info("no_new_articles")
                return
            
            # 2. è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ç« 
            new_articles = [
                a for a in articles 
                if not self.db.article_exists(a.url)
            ]
            
            if not new_articles:
                logger.info("all_articles_exist", total=len(articles))
                return
            
            logger.info("new_articles_found", count=len(new_articles))
            
            # 3. åº”ç”¨æ•°é‡é™åˆ¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            if limit and limit > 0:
                new_articles = new_articles[:limit]
                logger.info("articles_limited", limit=limit, count=len(new_articles))
            
            # 4. æå–æ­£æ–‡
            articles_with_content = await self.content_extractor.extract_all(new_articles)
            
            # 5. è½¬æ¢ä¸ºæ–°çš„ Article æ¨¡å‹
            new_format_articles = [
                self._convert_to_new_article(a) for a in articles_with_content
            ]
            
            # 5. ğŸ†• å¤šæ™ºèƒ½ä½“åˆ†æ (Legacy Flow - DISABLED)
            # enriched_articles = await self.orchestrator.analyze_batch(
            #     new_format_articles,
            #     mode=self.analysis_mode,
            #     max_concurrent=3,
            # )
            enriched_articles = []
            
            # 6. ä¿å­˜åˆ°æ•°æ®åº“ (Legacy Flow - DISABLED)
            # for article in enriched_articles:
            #     legacy_article = self._convert_to_legacy_article(article)
            #     self.db.save_analyzed_article(legacy_article)
            
            # 7. ğŸ†• ä¿¡æ¯ä¸ºä¸­å¿ƒçš„å¤„ç†æµç¨‹ (Beta) - å¹¶å‘ç‰ˆæœ¬
            logger.info("starting_info_centric_processing")
            info_processing_count = 0
            
            # å¹¶å‘æ§åˆ¶ï¼šä½¿ç”¨ Semaphore é™åˆ¶åŒæ—¶å¤„ç†çš„æ–‡ç« æ•°é‡
            CONCURRENT_LIMIT = self.concurrency  # ä½¿ç”¨å®ä¾‹é…ç½®çš„å¹¶å‘æ•°
            semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)
            logger.info("concurrent_processing_enabled", workers=CONCURRENT_LIMIT)
            
            async def process_single_article(article):
                """å¸¦ä¿¡å·é‡æ§åˆ¶çš„å•ç¯‡æ–‡ç« å¤„ç†"""
                async with semaphore:
                    # å…ˆä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“ï¼ˆç¡®ä¿ä¸ä¸¢å¤±ï¼‰
                    self.db.save_article(article)
                    # æ·±åº¦åˆ†æå¹¶æå–ä¿¡æ¯å•å…ƒ
                    units = await self.orchestrator.process_article_information_centric(article)
                    return len(units)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡å¹¶å¹¶å‘æ‰§è¡Œ
            tasks = [process_single_article(article) for article in new_format_articles]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡æˆåŠŸå¤„ç†çš„æ•°é‡
            for result in results:
                if isinstance(result, int):
                    info_processing_count += result
                elif isinstance(result, Exception):
                    logger.error("concurrent_article_failed", error=str(result))
            
            logger.info("fetch_cycle_complete",
                       fetched=len(articles),
                       new=len(new_articles),
                       analyzed=len(enriched_articles),
                       info_units_created=info_processing_count,
                       top_picks=sum(1 for a in enriched_articles if a.is_top_pick))
        
        except Exception as e:
            logger.error("fetch_cycle_failed", error=str(e))
            import traceback
            traceback.print_exc()
    
    async def send_daily_digest(self):
        """å‘é€æ¯æ—¥ç®€æŠ¥ï¼ˆä½¿ç”¨ AI æ™ºèƒ½ç­›é€‰ï¼‰"""
        logger.info("preparing_daily_digest")
        
        try:
            # Check for unsent information units FIRST (New Architecture)
            # ğŸ†• ä½¿ç”¨ Curator AI æ™ºèƒ½ç­›é€‰
            # ä¼˜å…ˆå°è¯•åŸºäº Information Units çš„ç®€æŠ¥
            from src.agents import CuratorAgent, InformationCuratorAgent
            from src.services.llm import LLMService

            unsent_units = self.info_store.get_unsent_units(limit=50)
            
            if len(unsent_units) >= 1: # Even 1 is enough for a digest if manual limit/once
                 # Use Information Centric Curation logic directly
                 pass # Logic continues below
            else:
                 # Check Legacy DB
                # è·å–æœªå‘é€çš„æ–‡ç« 
                db_articles = self.db.get_unsent_articles(
                    limit=self.config.filter.max_articles_per_digest
                )
                
                if not db_articles:
                    logger.info("no_articles_to_send")
                    return
                
                logger.info("articles_for_curation", count=len(db_articles))
            
                # è½¬æ¢ä¸º EnrichedArticle æ ¼å¼ä¾› Curator ä½¿ç”¨
                enriched_articles = []
                for a in db_articles:
                    enriched = EnrichedArticle(
                        url=a.url,
                        title=a.title,
                        content=a.content or "",
                        summary=a.summary or "",
                        source=a.source,
                        category=a.category,
                        overall_score=a.score or 5.0,
                        ai_summary=a.ai_summary or "",
                        is_top_pick=a.is_top_pick,
                        tags=a.tags or [],
                    )
                    enriched_articles.append(enriched)
            
            # Check for unsent information units
            # unsent_units variable is already set above
            
            if len(unsent_units) >= 1: # Lowered threshold from 5 to 1 for flexibility
                # Use Information Centric Curation
                logger.info("generating_info_centric_digest", units=len(unsent_units))
                info_curator = InformationCuratorAgent(LLMService(self.config.ai))
                curation_result = await info_curator.curate(
                    unsent_units,
                    max_top_picks=self.config.filter.max_articles_per_digest
                )
                
                # Build digest from Information Curation
                top_picks = []
                for item in curation_result["top_picks"]:
                    # Format as HTML for email
                    presentation = item.get("presentation", {})
                    summary_html = f"""
                    <div style="margin-bottom: 8px;"><strong>ğŸ“ äº‹å®æ‘˜è¦ï¼š</strong>{presentation.get('summary', '')}</div>
                    <div style="margin-bottom: 8px; color: #4b5563;"><strong>ğŸ’¡ æ·±åº¦åˆ†æï¼š</strong>{presentation.get('analysis', '')}</div>
                    <div style="color: #ea580c;"><strong>ğŸŒŠ æ½œåœ¨å½±å“ï¼š</strong>{presentation.get('impact', '')}</div>
                    """
                    
                    top_picks.append(DigestArticle(
                        title=item.get("display_title", ""),
                        url=self._get_unit_url(item.get("id"), unsent_units), # Helper needed
                        source=" | ".join(self._get_unit_sources(item.get("id"), unsent_units)),
                        category="æ·±åº¦ç²¾é€‰",
                        score=item.get("score", (item.get("reasoning", {}).get("score", 9.0) if isinstance(item.get("reasoning"), dict) else 9.0)),
                        summary=summary_html,
                        reasoning=item.get("reasoning", "") if isinstance(item.get("reasoning"), str) else "",
                        is_top_pick=True,
                        tags=[], # Tags not in output yet, maybe skip or fetch
                    ))
                    
                other_articles = []
                for item in curation_result["quick_reads"]:
                    other_articles.append(DigestArticle(
                        title=item.get("display_title", ""),
                        url=self._get_unit_url(item.get("id"), unsent_units),
                        source="å¿«é€Ÿæµè§ˆ",
                        category="èµ„è®¯",
                        score=7.0,
                        summary=item.get("one_line_summary", ""),
                        reasoning="",
                        is_top_pick=False,
                        tags=[],
                    ))
                    
                # Mark units as sent
                sent_ids = [item.get("id") for item in curation_result["top_picks"] + curation_result["quick_reads"]]
                self.info_store.mark_units_sent(sent_ids)
                
                # Create Digest Object
                digest = DailyDigest(
                    date=datetime.now(),
                    top_picks=top_picks,
                    other_articles=other_articles,
                    total_fetched=len(unsent_units), # Approx
                    total_analyzed=len(unsent_units),
                    total_filtered=len(top_picks) + len(other_articles),
                )
            
            else:
                # Fallback to Old Article-Centric Curation
                logger.info("fallback_to_article_curation", reason="not_enough_info_units")
                
                curator = CuratorAgent(LLMService(self.config.ai))
                curation_result = await curator.curate(
                    enriched_articles,
                    max_articles=self.config.filter.max_articles_per_digest,
                )
            
                # æ„å»ºç®€æŠ¥ (Old Logic)
                top_picks = []
                for article in curation_result["top_picks"]:
                    top_picks.append(DigestArticle(
                        title=article.title,
                        url=article.url,
                        source=article.source,
                        category=article.category,
                        score=article.overall_score,
                        summary=article.ai_summary or article.summary,
                        reasoning="",
                        is_top_pick=True,
                        tags=article.tags,
                    ))
                
                other_articles = []
                for article in curation_result["quick_reads"]:
                    other_articles.append(DigestArticle(
                        title=article.title,
                        url=article.url,
                        source=article.source,
                        category=article.category,
                        score=article.overall_score,
                        summary=article.ai_summary or article.summary,
                        reasoning="",
                        is_top_pick=False,
                        tags=article.tags,
                    ))
                
                digest = DailyDigest(
                    date=datetime.now(),
                    top_picks=top_picks,
                    other_articles=other_articles,
                    total_fetched=len(db_articles),
                    total_analyzed=len(db_articles),
                    total_filtered=len(top_picks) + len(other_articles),
                )
                
                # Mark Articles Sent (Old Logic)
                if True: # Will be handled below
                    sent_urls = [a.url for a in top_picks + other_articles]
                    self.db.mark_articles_sent(sent_urls)
            
            logger.info(
                "curation_complete",
                top_picks=len(top_picks),
                quick_reads=len(other_articles),
                excluded=len(curation_result.get("excluded", [])),
                daily_summary=curation_result.get("daily_summary", "")[:100],
            )
            
            # å‘é€é‚®ä»¶
            # å‘é€é‚®ä»¶
            success = await self.email_sender.send_digest(digest)
            
            if success:
                logger.info("digest_sent_successfully",
                           top_picks=len(top_picks),
                           other=len(other_articles))
            else:
                logger.error("digest_send_failed")
                

        
        except Exception as e:
            logger.error("digest_preparation_failed", error=str(e))
            import traceback
            traceback.print_exc()
    
    async def run_once(self, dry_run: bool = False, limit: int = None):
        """è¿è¡Œä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            dry_run: æ˜¯å¦ä¸å‘é€é‚®ä»¶
            limit: é™åˆ¶åˆ†æçš„æ–‡ç« æ•°é‡
        """
        await self.fetch_and_analyze(limit=limit)
        
        if not dry_run:
            await self.send_daily_digest()
    
    async def run(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("starting_service", mode=self.analysis_mode.value)
        self._running = True
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        self.scheduler.add_fetch_job(self.fetch_and_analyze)
        self.scheduler.add_digest_job(self.send_daily_digest)
        
        # å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡æŠ“å–
        await self.fetch_and_analyze()
        
        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()
        
        # ä¿æŒè¿è¡Œ
        try:
            while self._running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            pass
        finally:
            self.scheduler.stop()
            self.content_extractor.close()
            logger.info("service_stopped")
    
    def stop(self):
        """åœæ­¢æœåŠ¡"""
        self._running = False
    
    def _convert_to_new_article(self, legacy: LegacyArticle) -> NewArticle:
        """å°†æ—§ç‰ˆ Article è½¬æ¢ä¸ºæ–°ç‰ˆ"""
        return NewArticle(
            url=legacy.url,
            title=legacy.title,
            content=legacy.content,
            summary=legacy.summary,
            source=legacy.source,
            category=legacy.category,
            author=legacy.author,
            published_at=legacy.published_at,
            fetched_at=legacy.fetched_at,
        )
    
    def _convert_to_legacy_article(self, enriched: EnrichedArticle) -> AnalyzedArticle:
        """å°† EnrichedArticle è½¬æ¢ä¸ºæ—§ç‰ˆ AnalyzedArticleï¼ˆç”¨äºæ•°æ®åº“å­˜å‚¨ï¼‰"""
        return AnalyzedArticle(
            url=enriched.url,
            title=enriched.title,
            content=enriched.content,
            summary=enriched.summary,
            source=enriched.source,
            category=enriched.category,
            author=enriched.author,
            published_at=enriched.published_at,
            fetched_at=enriched.fetched_at,
            score=enriched.overall_score,
            ai_summary=enriched.ai_summary,
            is_top_pick=enriched.is_top_pick,
            reasoning=self._build_reasoning(enriched),
            tags=enriched.tags,
        )
    
    def _build_reasoning(self, enriched: EnrichedArticle) -> str:
        """ä» EnrichedArticle æ„å»ºæ¨ç†æ‘˜è¦"""
        parts = []
        
        # å¯ä¿¡åº¦
        if enriched.source_credibility:
            parts.append(f"ä¿¡æº: {enriched.source_credibility.tier}")
        
        # å½±å“
        if enriched.impact_analysis and enriched.impact_analysis.direct_impact:
            parts.append(f"ç›´æ¥å½±å“: {len(enriched.impact_analysis.direct_impact)}é¡¹")
        
        # å¸‚åœºæƒ…ç»ª
        if enriched.market_sentiment:
            parts.append(f"å¸‚åœº: {enriched.market_sentiment.overall}")
        
        # é£é™©
        if enriched.risk_warnings:
            parts.append(f"é£é™©è­¦ç¤º: {len(enriched.risk_warnings)}é¡¹")
        
        return " | ".join(parts) if parts else ""

    def _get_unit_url(self, unit_id: str, units: list) -> str:
        """Helper to get primary URL from unit ID"""
        for u in units:
            if u.id == unit_id:
                return u.primary_source
        return "#"

    def _get_unit_sources(self, unit_id: str, units: list) -> list:
        """Helper to get source names"""
        for u in units:
            if u.id == unit_id:
                return list(set(s.source_name for s in u.sources))[:3]
        return []


    async def run_backfill(self, limit: int = 100):
        """è¿è¡Œå®ä½“å›å¡«"""
        from src.agents import EntityBackfillAgent
        from src.services.llm import LLMService
        
        logger.info("starting_entity_backfill", limit=limit)
        
        backfill_agent = EntityBackfillAgent(
            llm_service=LLMService(self.config.ai),
            info_store=self.info_store,
            entity_store=self.entity_store
        )
        
        await backfill_agent.run(limit=limit)

    def run_query(self, query: str):
        """æŸ¥è¯¢å®ä½“ä¿¡æ¯"""
        print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢: {query} ...")
        # å°è¯•ç²¾ç¡®åŒ¹é…
        entity = self.entity_store.get_entity_by_name(query)
        
        if not entity:
            # å°è¯•æ¨¡ç³Šæœç´¢
            candidates = self.entity_store.search_entities(query)
            if candidates:
                print(f"â“ æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ã€‚æ‚¨æ˜¯æŒ‡: {', '.join([e.canonical_name for e in candidates])} ?")
            else:
                print("âŒ æœªæ‰¾åˆ°åä¸º '{}' çš„å®ä½“".format(query))
            return
            
        print("\n" + "="*60)
        print(f"ğŸ“ {entity.canonical_name}  [{entity.type.value}]")
        print("-" * 60)
        print(f"ğŸ“Š æåŠæ¬¡æ•°: {entity.mention_count}")
        print(f"ğŸ“… é¦–æ¬¡æåŠ: {entity.first_mentioned}")
        print(f"ğŸ“… æœ€è¿‘æåŠ: {entity.last_mentioned}")
        
        # åˆ«å
        aliases = self.entity_store.get_aliases(entity.id)
        if aliases:
            print(f"ğŸ·ï¸  åˆ«å: {', '.join(aliases)}")
        
        # å…³ç³»
        relations = self.entity_store.get_relations(entity.id)
        if relations:
            print(f"\nğŸ”— å…³ç³»ç½‘ç»œ ({len(relations)}):")
            for r in relations:
                other_id = r.target_id if r.source_id == entity.id else r.source_id
                other = self.entity_store.get_entity(other_id)
                other_name = other.canonical_name if other else "Unknown"
                
                direction = "â¡ï¸ " if r.source_id == entity.id else "â¬…ï¸ "
                rel_name = r.relation_type.value
                print(f"  {direction} {rel_name:<12} : {other_name} (ç½®ä¿¡åº¦:{r.confidence})")
        
        # æœ€è¿‘æåŠ
        mentions = self.entity_store.get_mentions_by_entity(entity.id, limit=5)
        if mentions:
            print(f"\nğŸ“ æœ€è¿‘æåŠ:")
            for m in mentions:
                print(f"  â€¢ {m.event_time}: [{m.role}] {m.state_dimension or ''} {m.state_delta or ''}")
                
                print(f"  â€¢ {m.event_time}: [{m.role}] {m.state_dimension or ''} {m.state_delta or ''}")
                
        print("="*60 + "\n")

    def run_visualize(self, output: str = "data/knowledge_graph.html"):
        """ç”ŸæˆçŸ¥è¯†å›¾è°±å¯è§†åŒ–"""
        from src.visualization import generate_knowledge_graph_html
        path = generate_knowledge_graph_html(self.entity_store, output)
        print(f"âœ… å¯è§†åŒ–å›¾è°±å·²ç”Ÿæˆ: {path}")
        print(f"ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ­¤æ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼å›¾è°±")

    async def run_reprocess(self, limit: int = 100):
        """
        é‡æ–°å¤„ç†ã€Œå·²ä¿å­˜ä½†æœªç”Ÿæˆ unitsã€çš„æ–‡ç« 
        
        è¿™äº›æ–‡ç« å¯èƒ½å› ä¸º LLM è¶…æ—¶ã€ç½‘ç»œé”™è¯¯ç­‰åŸå› å¯¼è‡´åˆ†æå¤±è´¥
        """
        print(f"ğŸ”„ æ­£åœ¨æŸ¥æ‰¾éœ€è¦é‡æ–°å¤„ç†çš„æ–‡ç« ...")
        
        # æŸ¥è¯¢ articles è¡¨ä¸­å­˜åœ¨ä½† information_units è¡¨ä¸­æ²¡æœ‰å¯¹åº”è®°å½•çš„æ–‡ç« 
        with self.db._get_conn() as conn:
            cursor = conn.execute("""
                SELECT a.url, a.title, a.content, a.source, a.published_at, a.fetched_at
                FROM articles a
                LEFT JOIN information_units u ON a.url = u.primary_source
                WHERE u.id IS NULL
                ORDER BY a.fetched_at DESC
                LIMIT ?
            """, (limit,))
            orphaned_articles = cursor.fetchall()
        
        if not orphaned_articles:
            print("âœ… æ²¡æœ‰éœ€è¦é‡æ–°å¤„ç†çš„æ–‡ç« ")
            return
            
        print(f"ğŸ“‹ æ‰¾åˆ° {len(orphaned_articles)} ç¯‡å¾…é‡æ–°å¤„ç†çš„æ–‡ç« ")
        logger.info("reprocess_started", count=len(orphaned_articles))
        
        # è½¬æ¢ä¸º Article å¯¹è±¡
        from src.models.article import Article
        articles_to_process = []
        for row in orphaned_articles:
            article = Article(
                url=row['url'],
                title=row['title'],
                content=row['content'] or "",
                source=row['source'],
                published_at=row['published_at'],
                fetched_at=row['fetched_at'],
            )
            articles_to_process.append(article)
        
        # ä½¿ç”¨å¹¶å‘å¤„ç†
        import asyncio
        semaphore = asyncio.Semaphore(self.concurrency)
        success_count = 0
        
        async def process_one(article):
            nonlocal success_count
            async with semaphore:
                try:
                    units = await self.orchestrator.process_article_information_centric(article)
                    if units:
                        success_count += 1
                        print(f"  âœ… {article.title[:40]}... ({len(units)} units)")
                    else:
                        print(f"  âš ï¸  {article.title[:40]}... (0 units)")
                    return len(units)
                except Exception as e:
                    print(f"  âŒ {article.title[:40]}... Error: {str(e)[:50]}")
                    logger.error("reprocess_failed", url=article.url, error=str(e))
                    return 0
        
        tasks = [process_one(article) for article in articles_to_process]
        await asyncio.gather(*tasks)
        
        print(f"\nğŸ‰ é‡æ–°å¤„ç†å®Œæˆ: {success_count}/{len(orphaned_articles)} ç¯‡æˆåŠŸ")
        logger.info("reprocess_completed", success=success_count, total=len(orphaned_articles))


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="RSS AI Reader - å¤šæ™ºèƒ½ä½“æ™ºèƒ½ RSS é˜…è¯»å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m src.main                         # å¯åŠ¨æœåŠ¡ï¼ˆæ·±åº¦åˆ†ææ¨¡å¼ï¼‰
  python -m src.main --mode quick            # å¿«é€Ÿåˆ†ææ¨¡å¼
  python -m src.main --mode standard         # æ ‡å‡†åˆ†ææ¨¡å¼
  python -m src.main --once                  # è¿è¡Œä¸€æ¬¡ï¼ˆåˆ†ææ‰€æœ‰æ–°æ–‡ç« ï¼‰
  python -m src.main --once --limit 1        # åªåˆ†æ 1 ç¯‡ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
  python -m src.main --once --limit 5        # åªåˆ†æå‰ 5 ç¯‡
  python -m src.main --once --dry-run        # æµ‹è¯•è¿è¡Œï¼ˆä¸å‘é€é‚®ä»¶ï¼‰
  python -m src.main --test-email            # å‘é€æµ‹è¯•é‚®ä»¶
        """
    )
    
    parser.add_argument(
        "--mode", "-m",
        type=str,
        choices=["quick", "standard", "deep"],
        default="deep",
        help="åˆ†ææ¨¡å¼: quick(å¿«é€Ÿ), standard(æ ‡å‡†), deep(æ·±åº¦)"
    )
    
    parser.add_argument(
        "--once", "-1",
        action="store_true",
        help="åªè¿è¡Œä¸€æ¬¡ï¼Œç„¶åé€€å‡º"
    )
    
    parser.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼Œä¸å‘é€é‚®ä»¶"
    )
    
    parser.add_argument(
        "--test-email", "-t",
        action="store_true",
        help="å‘é€æµ‹è¯•é‚®ä»¶"
    )
    
    parser.add_argument(
        "--config-dir", "-c",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶ç›®å½•"
    )
    
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=None,
        help="é™åˆ¶åˆ†æçš„æ–‡ç« æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    
    parser.add_argument(
        "--backfill",
        action="store_true",
        help="è¿è¡Œå®ä½“å›å¡«ä»»åŠ¡"
    )
    
    parser.add_argument(
        "--concurrency", "-j",
        type=int,
        default=5,
        help="å¹¶å‘å¤„ç†æ–‡ç« æ•°é‡ (é»˜è®¤: 5)"
    )
    
    
    parser.add_argument(
        "--query", "-q",
        type=str,
        help="æŸ¥è¯¢å®ä½“çŸ¥è¯†å›¾è°± (è¾“å…¥å®ä½“åç§°)"
    )
    
    parser.add_argument(
        "--visualize",
        action="store_true",
        help="ç”ŸæˆçŸ¥è¯†å›¾è°±å¯è§†åŒ– HTML"
    )
    
    parser.add_argument(
        "--reprocess",
        action="store_true",
        help="é‡æ–°å¤„ç†ã€Œå·²ä¿å­˜ä½†æœªç”Ÿæˆ unitsã€çš„æ–‡ç« "
    )
    
    parser.add_argument(
        "--digest",
        action="store_true",
        help="ç›´æ¥å‘é€æ¯æ—¥æ‘˜è¦é‚®ä»¶ï¼ˆä¸æŠ“å–æ–°æ–‡ç« ï¼‰"
    )
    
    # æ·»åŠ  telemetry å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # telemetry å‘½ä»¤
    tele_parser = subparsers.add_parser("telemetry", help="AI é¥æµ‹ç®¡ç†")
    tele_subparsers = tele_parser.add_subparsers(dest="tele_action", help="é¥æµ‹æ“ä½œ")
    
    # telemetry stats
    stats_parser = tele_subparsers.add_parser("stats", help="æ˜¾ç¤ºé¥æµ‹ç»Ÿè®¡")
    stats_parser.add_argument("--days", "-d", type=int, default=7, help="ç»Ÿè®¡å¤©æ•°")
    
    # telemetry list
    list_parser = tele_subparsers.add_parser("list", help="åˆ—å‡ºæœ€è¿‘çš„ AI è°ƒç”¨")
    list_parser.add_argument("--limit", "-l", type=int, default=20, help="æ˜¾ç¤ºæ•°é‡")
    list_parser.add_argument("--session", "-s", type=str, help="æŒ‰ session è¿‡æ»¤")
    list_parser.add_argument("--agent", "-a", type=str, help="æŒ‰ agent è¿‡æ»¤")
    
    # telemetry export
    export_parser = tele_subparsers.add_parser("export", help="å¯¼å‡ºé¥æµ‹æ•°æ®")
    export_parser.add_argument("--output", "-o", type=str, default="telemetry_export.jsonl", help="è¾“å‡ºæ–‡ä»¶")
    export_parser.add_argument("--days", "-d", type=int, default=7, help="å¯¼å‡ºå¤©æ•°")
    
    # telemetry cleanup
    cleanup_parser = tele_subparsers.add_parser("cleanup", help="æ¸…ç†è¿‡æœŸé¥æµ‹æ•°æ®")
    
    # telemetry sessions
    sessions_parser = tele_subparsers.add_parser("sessions", help="åˆ—å‡ºè¿½è¸ªä¼šè¯")
    sessions_parser.add_argument("--limit", "-l", type=int, default=20, help="æ˜¾ç¤ºæ•°é‡")
    
    return parser.parse_args()


def handle_telemetry_command(args, config):
    """å¤„ç†é¥æµ‹ç›¸å…³å‘½ä»¤"""
    from datetime import datetime, timedelta
    from src.services.telemetry import AITelemetry
    import json
    
    # åˆå§‹åŒ–é¥æµ‹
    telemetry = AITelemetry.initialize(
        enabled=config.telemetry.enabled,
        storage_path=config.telemetry.storage_path,
        retention_days=config.telemetry.retention_days,
    )
    
    if args.tele_action == "stats":
        # ç»Ÿè®¡ä¿¡æ¯
        days = args.days
        start_time = datetime.now() - timedelta(days=days)
        stats = telemetry.get_stats(start_time=start_time)
        
        print(f"\nğŸ“Š AI é¥æµ‹ç»Ÿè®¡ (æœ€è¿‘ {days} å¤©)")
        print("=" * 40)
        print(f"æ€»è°ƒç”¨æ¬¡æ•°: {stats.total_calls}")
        print(f"æ€» Token ä½¿ç”¨: {stats.total_tokens:,}")
        print(f"  - Prompt: {stats.total_prompt_tokens:,}")
        print(f"  - Completion: {stats.total_completion_tokens:,}")
        print(f"æ€»è€—æ—¶: {stats.total_duration_ms / 1000:.1f} ç§’")
        print(f"å¹³å‡è€—æ—¶: {stats.avg_duration_ms:.0f} æ¯«ç§’/æ¬¡")
        print(f"é”™è¯¯æ¬¡æ•°: {stats.error_count} ({stats.error_rate:.1f}%)")
        
        if stats.calls_by_type:
            print(f"\næŒ‰ç±»å‹åˆ†å¸ƒ:")
            for call_type, count in stats.calls_by_type.items():
                print(f"  {call_type}: {count}")
        
        if stats.calls_by_agent:
            print(f"\næŒ‰ Agent åˆ†å¸ƒ:")
            for agent, count in sorted(stats.calls_by_agent.items(), key=lambda x: -x[1])[:10]:
                print(f"  {agent or 'N/A'}: {count}")
    
    elif args.tele_action == "list":
        # åˆ—å‡ºè°ƒç”¨
        records = telemetry.query(
            limit=args.limit,
            session_id=getattr(args, 'session', None),
            agent_name=getattr(args, 'agent', None),
        )
        
        print(f"\nğŸ“‹ æœ€è¿‘ {len(records)} æ¡ AI è°ƒç”¨è®°å½•")
        print("=" * 80)
        for r in records:
            ts = r['timestamp'][:19] if r.get('timestamp') else 'N/A'
            agent = r.get('agent_name') or 'N/A'
            print(f"{ts} | {r['call_type']:<10} | {agent:<20} | {r['total_tokens']:>6} tokens | {r['duration_ms']:>5}ms")
    
    elif args.tele_action == "export":
        # å¯¼å‡ºæ•°æ®
        days = args.days
        start_time = datetime.now() - timedelta(days=days)
        output = args.output
        
        count = telemetry.export(output, start_time=start_time)
        print(f"\nâœ… å·²å¯¼å‡º {count} æ¡è®°å½•åˆ° {output}")
    
    elif args.tele_action == "cleanup":
        # æ¸…ç†æ•°æ®
        deleted = telemetry.cleanup()
        print(f"\nâœ… å·²æ¸…ç† {deleted} æ¡è¿‡æœŸè®°å½•")
    
    elif args.tele_action == "sessions":
        # åˆ—å‡º session
        sessions = telemetry.list_sessions(limit=args.limit)
        
        print(f"\nğŸ“ æœ€è¿‘ {len(sessions)} ä¸ªè¿½è¸ªä¼šè¯")
        print("=" * 80)
        for s in sessions:
            print(f"{s['session_id']} | {s['call_count']:>3} calls | {s['total_tokens'] or 0:>6} tokens | {s['start_time'][:19]}")
    
    else:
        print("è¯·æŒ‡å®šæ“ä½œ: stats, list, export, cleanup, sessions")
        print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    if args.config_dir:
        config = reload_config(Path(args.config_dir))
    else:
        config = get_config()
    
    # å¤„ç† telemetry å­å‘½ä»¤
    if args.command == "telemetry":
        handle_telemetry_command(args, config)
        return
    
    # åˆ›å»ºæœåŠ¡
    service = RSSReaderService(config, analysis_mode=args.mode, concurrency=args.concurrency)
    
    # å¤„ç†ä¿¡å·
    loop = asyncio.get_event_loop()
    
    def signal_handler():
        logger.info("received_shutdown_signal")
        service.stop()
    
    for sig in (signal.SIGTERM, signal.SIGINT):
        loop.add_signal_handler(sig, signal_handler)
    
    # è¿è¡Œæ¨¡å¼
    if args.test_email:
        # æµ‹è¯•é‚®ä»¶
        email_sender = EmailSender(config.email)
        success = await email_sender.send_test_email()
        if success:
            print("âœ… æµ‹è¯•é‚®ä»¶å‘é€æˆåŠŸï¼")
        else:
            print("âŒ æµ‹è¯•é‚®ä»¶å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            sys.exit(1)
    
            
    elif args.backfill:
        # å®ä½“å›å¡«
        print(f"ğŸ”„ å¼€å§‹å®ä½“å›å¡« (Limit: {args.limit or 100})...")
        await service.run_backfill(limit=args.limit or 100)
        print("âœ… å›å¡«å®Œæˆï¼")
        
    elif args.query:
        # æŸ¥è¯¢å®ä½“
        service.run_query(args.query)
        
    elif args.visualize:
        # å¯è§†åŒ–
        service.run_visualize()
    
    elif args.reprocess:
        # é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ç« 
        print(f"ğŸ”„ å¼€å§‹é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ç«  (Limit: {args.limit or 100})...")
        await service.run_reprocess(limit=args.limit or 100)
    
    elif args.digest:
        # ç›´æ¥å‘é€æ¯æ—¥æ‘˜è¦
        print("ğŸ“§ æ­£åœ¨å‘é€æ¯æ—¥æ‘˜è¦...")
        await service.send_daily_digest()
        print("âœ… æ‘˜è¦å‘é€å®Œæˆï¼")
    
    elif args.once:
        # è¿è¡Œä¸€æ¬¡
        mode_names = {"quick": "å¿«é€Ÿ", "standard": "æ ‡å‡†", "deep": "æ·±åº¦"}
        print(f"ğŸ” ä½¿ç”¨ {mode_names[args.mode]} åˆ†ææ¨¡å¼...")
        if args.limit:
            print(f"ğŸ“Š é™åˆ¶åˆ†ææ•°é‡: {args.limit} ç¯‡")
        await service.run_once(dry_run=args.dry_run, limit=args.limit)
        print("âœ… è¿è¡Œå®Œæˆï¼")
    
    else:
        # æŒç»­è¿è¡Œ
        mode_names = {"quick": "å¿«é€Ÿ", "standard": "æ ‡å‡†", "deep": "æ·±åº¦"}
        print("ğŸš€ RSS AI Reader æœåŠ¡å·²å¯åŠ¨ï¼ˆå¤šæ™ºèƒ½ä½“ç‰ˆæœ¬ï¼‰")
        print(f"ğŸ§  åˆ†ææ¨¡å¼: {mode_names[args.mode]}")
        print(f"ğŸ“¥ æŠ“å–é—´éš”: {config.schedule.fetch_interval}")
        digest_times_str = "ã€".join(config.schedule.digest_times)
        print(f"ğŸ“§ ç®€æŠ¥æ—¶é—´: æ¯å¤© {digest_times_str}")
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡...")
        await service.run()


def main():
    """ä¸»å…¥å£"""
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()
