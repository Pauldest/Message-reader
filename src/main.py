"""RSS AI Reader - ä¸»ç¨‹åºå…¥å£ (Multi-Agent Version)"""

import argparse
import asyncio
import signal
import sys
from datetime import datetime
from pathlib import Path
import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ]
)

from .config import get_config, reload_config, AppConfig
from .fetcher import RSSParser, ContentExtractor
from .agents import AnalysisOrchestrator
from .models.agent import AnalysisMode
from .models.article import Article as NewArticle, EnrichedArticle
from .storage import Database, DigestArticle, DailyDigest
from .storage.models import Article as LegacyArticle, AnalyzedArticle
from .notifier import EmailSender
from .scheduler import Scheduler

logger = structlog.get_logger()


class RSSReaderService:
    """RSS é˜…è¯»å™¨æœåŠ¡ (Multi-Agent Version)"""
    
    def __init__(self, config: AppConfig, analysis_mode: str = "deep"):
        self.config = config
        
        # è§£æåˆ†ææ¨¡å¼
        self.analysis_mode = AnalysisMode(analysis_mode)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db = Database(config.storage.database_path)
        self.rss_parser = RSSParser()
        self.content_extractor = ContentExtractor()
        
        # ğŸ†• å¤šæ™ºèƒ½ä½“åˆ†æå™¨
        self.orchestrator = AnalysisOrchestrator(config)
        
        self.email_sender = EmailSender(config.email)
        self.scheduler = Scheduler(config.schedule)
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
        
        logger.info(
            "service_initialized",
            analysis_mode=self.analysis_mode.value,
            vector_store=self.orchestrator.get_stats().get("vector_store", {}),
        )
    
    async def fetch_and_analyze(self):
        """æŠ“å–å¹¶åˆ†ææ–‡ç« """
        logger.info("starting_fetch_cycle", mode=self.analysis_mode.value)
        
        try:
            # 1. æŠ“å– RSS
            articles = await self.rss_parser.fetch_all(self.config.feeds)
            
            if not articles:
                logger.info("no_new_articles")
                return
            
            # 2. è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ç« 
            new_articles = [
                a for a in articles 
                if not self.db.article_exists(a.url)
            ]
            
            if not new_articles:
                logger.info("all_articles_exist", total=len(articles))
                return
            
            logger.info("new_articles_found", count=len(new_articles))
            
            # 3. æå–æ­£æ–‡
            articles_with_content = await self.content_extractor.extract_all(new_articles)
            
            # 4. è½¬æ¢ä¸ºæ–°çš„ Article æ¨¡å‹
            new_format_articles = [
                self._convert_to_new_article(a) for a in articles_with_content
            ]
            
            # 5. ğŸ†• å¤šæ™ºèƒ½ä½“åˆ†æ
            enriched_articles = await self.orchestrator.analyze_batch(
                new_format_articles,
                mode=self.analysis_mode,
                max_concurrent=3,
            )
            
            # 6. ä¿å­˜åˆ°æ•°æ®åº“
            for article in enriched_articles:
                legacy_article = self._convert_to_legacy_article(article)
                self.db.save_analyzed_article(legacy_article)
            
            logger.info("fetch_cycle_complete",
                       fetched=len(articles),
                       new=len(new_articles),
                       analyzed=len(enriched_articles),
                       top_picks=sum(1 for a in enriched_articles if a.is_top_pick))
        
        except Exception as e:
            logger.error("fetch_cycle_failed", error=str(e))
            import traceback
            traceback.print_exc()
    
    async def send_daily_digest(self):
        """å‘é€æ¯æ—¥ç®€æŠ¥"""
        logger.info("preparing_daily_digest")
        
        try:
            # è·å–æœªå‘é€çš„æ–‡ç« 
            articles = self.db.get_unsent_articles(
                limit=self.config.filter.max_articles_per_digest
            )
            
            if not articles:
                logger.info("no_articles_to_send")
                return
            
            # æ„å»ºç®€æŠ¥
            top_picks = []
            other_articles = []
            
            for article in articles:
                digest_article = DigestArticle(
                    title=article.title,
                    url=article.url,
                    source=article.source,
                    category=article.category,
                    score=article.score,
                    summary=article.ai_summary or article.summary,
                    reasoning=article.reasoning,
                    is_top_pick=article.is_top_pick,
                    tags=article.tags,
                )
                
                if article.is_top_pick:
                    top_picks.append(digest_article)
                elif article.score >= self.config.filter.min_score:
                    other_articles.append(digest_article)
            
            # é™åˆ¶ç²¾é€‰æ•°é‡
            top_picks = top_picks[:self.config.filter.top_pick_count]
            
            digest = DailyDigest(
                date=datetime.now(),
                top_picks=top_picks,
                other_articles=other_articles,
                total_fetched=len(articles),
                total_analyzed=len(articles),
                total_filtered=len(top_picks) + len(other_articles),
            )
            
            # å‘é€é‚®ä»¶
            success = await self.email_sender.send_digest(digest)
            
            if success:
                # æ ‡è®°æ–‡ç« å·²å‘é€
                sent_urls = [a.url for a in articles]
                self.db.mark_articles_sent(sent_urls)
                logger.info("digest_sent_successfully",
                           top_picks=len(top_picks),
                           other=len(other_articles))
            else:
                logger.error("digest_send_failed")
        
        except Exception as e:
            logger.error("digest_preparation_failed", error=str(e))
    
    async def run_once(self, dry_run: bool = False):
        """è¿è¡Œä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await self.fetch_and_analyze()
        
        if not dry_run:
            await self.send_daily_digest()
    
    async def run(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("starting_service", mode=self.analysis_mode.value)
        self._running = True
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        self.scheduler.add_fetch_job(self.fetch_and_analyze)
        self.scheduler.add_digest_job(self.send_daily_digest)
        
        # å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡æŠ“å–
        await self.fetch_and_analyze()
        
        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()
        
        # ä¿æŒè¿è¡Œ
        try:
            while self._running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            pass
        finally:
            self.scheduler.stop()
            self.content_extractor.close()
            logger.info("service_stopped")
    
    def stop(self):
        """åœæ­¢æœåŠ¡"""
        self._running = False
    
    def _convert_to_new_article(self, legacy: LegacyArticle) -> NewArticle:
        """å°†æ—§ç‰ˆ Article è½¬æ¢ä¸ºæ–°ç‰ˆ"""
        return NewArticle(
            url=legacy.url,
            title=legacy.title,
            content=legacy.content,
            summary=legacy.summary,
            source=legacy.source,
            category=legacy.category,
            author=legacy.author,
            published_at=legacy.published_at,
            fetched_at=legacy.fetched_at,
        )
    
    def _convert_to_legacy_article(self, enriched: EnrichedArticle) -> AnalyzedArticle:
        """å°† EnrichedArticle è½¬æ¢ä¸ºæ—§ç‰ˆ AnalyzedArticleï¼ˆç”¨äºæ•°æ®åº“å­˜å‚¨ï¼‰"""
        return AnalyzedArticle(
            url=enriched.url,
            title=enriched.title,
            content=enriched.content,
            summary=enriched.summary,
            source=enriched.source,
            category=enriched.category,
            author=enriched.author,
            published_at=enriched.published_at,
            fetched_at=enriched.fetched_at,
            score=enriched.overall_score,
            ai_summary=enriched.ai_summary,
            is_top_pick=enriched.is_top_pick,
            reasoning=self._build_reasoning(enriched),
            tags=enriched.tags,
        )
    
    def _build_reasoning(self, enriched: EnrichedArticle) -> str:
        """ä» EnrichedArticle æ„å»ºæ¨ç†æ‘˜è¦"""
        parts = []
        
        # å¯ä¿¡åº¦
        if enriched.source_credibility:
            parts.append(f"ä¿¡æº: {enriched.source_credibility.tier}")
        
        # å½±å“
        if enriched.impact_analysis and enriched.impact_analysis.direct_impact:
            parts.append(f"ç›´æ¥å½±å“: {len(enriched.impact_analysis.direct_impact)}é¡¹")
        
        # å¸‚åœºæƒ…ç»ª
        if enriched.market_sentiment:
            parts.append(f"å¸‚åœº: {enriched.market_sentiment.overall}")
        
        # é£é™©
        if enriched.risk_warnings:
            parts.append(f"é£é™©è­¦ç¤º: {len(enriched.risk_warnings)}é¡¹")
        
        return " | ".join(parts) if parts else ""


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="RSS AI Reader - å¤šæ™ºèƒ½ä½“æ™ºèƒ½ RSS é˜…è¯»å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m src.main                         # å¯åŠ¨æœåŠ¡ï¼ˆæ·±åº¦åˆ†ææ¨¡å¼ï¼‰
  python -m src.main --mode quick            # å¿«é€Ÿåˆ†ææ¨¡å¼
  python -m src.main --mode standard         # æ ‡å‡†åˆ†ææ¨¡å¼
  python -m src.main --once                  # è¿è¡Œä¸€æ¬¡
  python -m src.main --once --dry-run        # æµ‹è¯•è¿è¡Œï¼ˆä¸å‘é€é‚®ä»¶ï¼‰
  python -m src.main --test-email            # å‘é€æµ‹è¯•é‚®ä»¶
        """
    )
    
    parser.add_argument(
        "--mode", "-m",
        type=str,
        choices=["quick", "standard", "deep"],
        default="deep",
        help="åˆ†ææ¨¡å¼: quick(å¿«é€Ÿ), standard(æ ‡å‡†), deep(æ·±åº¦)"
    )
    
    parser.add_argument(
        "--once", "-1",
        action="store_true",
        help="åªè¿è¡Œä¸€æ¬¡ï¼Œç„¶åé€€å‡º"
    )
    
    parser.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼Œä¸å‘é€é‚®ä»¶"
    )
    
    parser.add_argument(
        "--test-email", "-t",
        action="store_true",
        help="å‘é€æµ‹è¯•é‚®ä»¶"
    )
    
    parser.add_argument(
        "--config-dir", "-c",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶ç›®å½•"
    )
    
    return parser.parse_args()


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    if args.config_dir:
        config = reload_config(Path(args.config_dir))
    else:
        config = get_config()
    
    # åˆ›å»ºæœåŠ¡
    service = RSSReaderService(config, analysis_mode=args.mode)
    
    # å¤„ç†ä¿¡å·
    loop = asyncio.get_event_loop()
    
    def signal_handler():
        logger.info("received_shutdown_signal")
        service.stop()
    
    for sig in (signal.SIGTERM, signal.SIGINT):
        loop.add_signal_handler(sig, signal_handler)
    
    # è¿è¡Œæ¨¡å¼
    if args.test_email:
        # æµ‹è¯•é‚®ä»¶
        email_sender = EmailSender(config.email)
        success = await email_sender.send_test_email()
        if success:
            print("âœ… æµ‹è¯•é‚®ä»¶å‘é€æˆåŠŸï¼")
        else:
            print("âŒ æµ‹è¯•é‚®ä»¶å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            sys.exit(1)
    
    elif args.once:
        # è¿è¡Œä¸€æ¬¡
        mode_names = {"quick": "å¿«é€Ÿ", "standard": "æ ‡å‡†", "deep": "æ·±åº¦"}
        print(f"ğŸ” ä½¿ç”¨ {mode_names[args.mode]} åˆ†ææ¨¡å¼...")
        await service.run_once(dry_run=args.dry_run)
        print("âœ… è¿è¡Œå®Œæˆï¼")
    
    else:
        # æŒç»­è¿è¡Œ
        mode_names = {"quick": "å¿«é€Ÿ", "standard": "æ ‡å‡†", "deep": "æ·±åº¦"}
        print("ğŸš€ RSS AI Reader æœåŠ¡å·²å¯åŠ¨ï¼ˆå¤šæ™ºèƒ½ä½“ç‰ˆæœ¬ï¼‰")
        print(f"ğŸ§  åˆ†ææ¨¡å¼: {mode_names[args.mode]}")
        print(f"ğŸ“¥ æŠ“å–é—´éš”: {config.schedule.fetch_interval}")
        digest_times_str = "ã€".join(config.schedule.digest_times)
        print(f"ğŸ“§ ç®€æŠ¥æ—¶é—´: æ¯å¤© {digest_times_str}")
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡...")
        await service.run()


def main():
    """ä¸»å…¥å£"""
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()
