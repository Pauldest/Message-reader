"""RSS AI Reader - ä¸»ç¨‹åºå…¥å£"""

import argparse
import asyncio
import signal
import sys
from datetime import datetime
from pathlib import Path
import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ]
)

from .config import get_config, reload_config, AppConfig
from .fetcher import RSSParser, ContentExtractor
from .ai import ArticleAnalyzer
from .storage import Database, AnalyzedArticle, DigestArticle, DailyDigest
from .notifier import EmailSender
from .scheduler import Scheduler

logger = structlog.get_logger()


class RSSReaderService:
    """RSS é˜…è¯»å™¨æœåŠ¡"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db = Database(config.storage.database_path)
        self.rss_parser = RSSParser()
        self.content_extractor = ContentExtractor()
        self.analyzer = ArticleAnalyzer(config.ai)
        self.email_sender = EmailSender(config.email)
        self.scheduler = Scheduler(config.schedule)
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
    
    async def fetch_and_analyze(self):
        """æŠ“å–å¹¶åˆ†ææ–‡ç« """
        logger.info("starting_fetch_cycle")
        
        try:
            # 1. æŠ“å– RSS
            articles = await self.rss_parser.fetch_all(self.config.feeds)
            
            if not articles:
                logger.info("no_new_articles")
                return
            
            # 2. è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ç« 
            new_articles = [
                a for a in articles 
                if not self.db.article_exists(a.url)
            ]
            
            if not new_articles:
                logger.info("all_articles_exist", total=len(articles))
                return
            
            logger.info("new_articles_found", count=len(new_articles))
            
            # 3. æå–æ­£æ–‡
            articles_with_content = await self.content_extractor.extract_all(new_articles)
            
            # 4. AI åˆ†æ
            analyzed = await self.analyzer.analyze_batch(
                articles_with_content,
                top_pick_count=self.config.filter.top_pick_count,
            )
            
            # 5. ä¿å­˜åˆ°æ•°æ®åº“
            for article in analyzed:
                self.db.save_analyzed_article(article)
            
            logger.info("fetch_cycle_complete",
                       fetched=len(articles),
                       new=len(new_articles),
                       analyzed=len(analyzed))
        
        except Exception as e:
            logger.error("fetch_cycle_failed", error=str(e))
    
    async def send_daily_digest(self):
        """å‘é€æ¯æ—¥ç®€æŠ¥"""
        logger.info("preparing_daily_digest")
        
        try:
            # è·å–æœªå‘é€çš„æ–‡ç« 
            articles = self.db.get_unsent_articles(
                limit=self.config.filter.max_articles_per_digest
            )
            
            if not articles:
                logger.info("no_articles_to_send")
                return
            
            # æ„å»ºç®€æŠ¥
            top_picks = []
            other_articles = []
            
            for article in articles:
                digest_article = DigestArticle(
                    title=article.title,
                    url=article.url,
                    source=article.source,
                    category=article.category,
                    score=article.score,
                    summary=article.ai_summary or article.summary,
                    reasoning=article.reasoning,
                    is_top_pick=article.is_top_pick,
                    tags=article.tags,
                )
                
                if article.is_top_pick:
                    top_picks.append(digest_article)
                elif article.score >= self.config.filter.min_score:
                    other_articles.append(digest_article)
            
            # é™åˆ¶ç²¾é€‰æ•°é‡
            top_picks = top_picks[:self.config.filter.top_pick_count]
            
            digest = DailyDigest(
                date=datetime.now(),
                top_picks=top_picks,
                other_articles=other_articles,
                total_fetched=len(articles),
                total_analyzed=len(articles),
                total_filtered=len(top_picks) + len(other_articles),
            )
            
            # å‘é€é‚®ä»¶
            success = await self.email_sender.send_digest(digest)
            
            if success:
                # æ ‡è®°æ–‡ç« å·²å‘é€
                sent_urls = [a.url for a in articles]
                self.db.mark_articles_sent(sent_urls)
                logger.info("digest_sent_successfully",
                           top_picks=len(top_picks),
                           other=len(other_articles))
            else:
                logger.error("digest_send_failed")
        
        except Exception as e:
            logger.error("digest_preparation_failed", error=str(e))
    
    async def run_once(self, dry_run: bool = False):
        """è¿è¡Œä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await self.fetch_and_analyze()
        
        if not dry_run:
            await self.send_daily_digest()
    
    async def run(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("starting_service")
        self._running = True
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        self.scheduler.add_fetch_job(self.fetch_and_analyze)
        self.scheduler.add_digest_job(self.send_daily_digest)
        
        # å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡æŠ“å–
        await self.fetch_and_analyze()
        
        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()
        
        # ä¿æŒè¿è¡Œ
        try:
            while self._running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            pass
        finally:
            self.scheduler.stop()
            self.content_extractor.close()
            logger.info("service_stopped")
    
    def stop(self):
        """åœæ­¢æœåŠ¡"""
        self._running = False


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="RSS AI Reader - æ™ºèƒ½ RSS é˜…è¯»å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m src.main                    # å¯åŠ¨æœåŠ¡
  python -m src.main --once             # è¿è¡Œä¸€æ¬¡
  python -m src.main --once --dry-run   # æµ‹è¯•è¿è¡Œï¼ˆä¸å‘é€é‚®ä»¶ï¼‰
  python -m src.main --test-email       # å‘é€æµ‹è¯•é‚®ä»¶
        """
    )
    
    parser.add_argument(
        "--once", "-1",
        action="store_true",
        help="åªè¿è¡Œä¸€æ¬¡ï¼Œç„¶åé€€å‡º"
    )
    
    parser.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼Œä¸å‘é€é‚®ä»¶"
    )
    
    parser.add_argument(
        "--test-email", "-t",
        action="store_true",
        help="å‘é€æµ‹è¯•é‚®ä»¶"
    )
    
    parser.add_argument(
        "--config-dir", "-c",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶ç›®å½•"
    )
    
    return parser.parse_args()


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    if args.config_dir:
        config = reload_config(Path(args.config_dir))
    else:
        config = get_config()
    
    # åˆ›å»ºæœåŠ¡
    service = RSSReaderService(config)
    
    # å¤„ç†ä¿¡å·
    loop = asyncio.get_event_loop()
    
    def signal_handler():
        logger.info("received_shutdown_signal")
        service.stop()
    
    for sig in (signal.SIGTERM, signal.SIGINT):
        loop.add_signal_handler(sig, signal_handler)
    
    # è¿è¡Œæ¨¡å¼
    if args.test_email:
        # æµ‹è¯•é‚®ä»¶
        email_sender = EmailSender(config.email)
        success = await email_sender.send_test_email()
        if success:
            print("âœ… æµ‹è¯•é‚®ä»¶å‘é€æˆåŠŸï¼")
        else:
            print("âŒ æµ‹è¯•é‚®ä»¶å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            sys.exit(1)
    
    elif args.once:
        # è¿è¡Œä¸€æ¬¡
        await service.run_once(dry_run=args.dry_run)
        print("âœ… è¿è¡Œå®Œæˆï¼")
    
    else:
        # æŒç»­è¿è¡Œ
        print("ğŸš€ RSS AI Reader æœåŠ¡å·²å¯åŠ¨")
        print(f"ğŸ“¥ æŠ“å–é—´éš”: {config.schedule.fetch_interval}")
        digest_times_str = "ã€".join(config.schedule.digest_times)
        print(f"ğŸ“§ ç®€æŠ¥æ—¶é—´: æ¯å¤© {digest_times_str}")
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡...")
        await service.run()


def main():
    """ä¸»å…¥å£"""
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()
