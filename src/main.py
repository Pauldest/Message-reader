"""RSS AI Reader - ä¸»ç¨‹åºå…¥å£ (Multi-Agent Version)"""

import argparse
import asyncio
import signal
import sys
from datetime import datetime
from pathlib import Path

# ç¡®ä¿å¯ä»¥æ‰¾åˆ° src åŒ…ï¼ˆæ”¯æŒç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼‰
if __name__ == "__main__":
    sys.path.insert(0, str(Path(__file__).parent.parent))

import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ]
)

# ä½¿ç”¨æ¡ä»¶å¯¼å…¥æ”¯æŒä¸¤ç§è¿è¡Œæ–¹å¼
try:
    # ä½œä¸ºæ¨¡å—è¿è¡Œ: python -m src.main
    from .config import get_config, reload_config, AppConfig
    from .fetcher import RSSParser, ContentExtractor
    from .agents import AnalysisOrchestrator
    from .models.agent import AnalysisMode
    from .models.article import Article as NewArticle, EnrichedArticle
    from .storage import Database, DigestArticle, DailyDigest
    from .storage.models import Article as LegacyArticle, AnalyzedArticle
    from .notifier import EmailSender
    from .scheduler import Scheduler
except ImportError:
    # ç›´æ¥è¿è¡Œ: python src/main.py
    from src.config import get_config, reload_config, AppConfig
    from src.fetcher import RSSParser, ContentExtractor
    from src.agents import AnalysisOrchestrator
    from src.models.agent import AnalysisMode
    from src.models.article import Article as NewArticle, EnrichedArticle
    from src.storage import Database, DigestArticle, DailyDigest
    from src.storage.models import Article as LegacyArticle, AnalyzedArticle
    from src.notifier import EmailSender
    from src.scheduler import Scheduler

logger = structlog.get_logger()


class RSSReaderService:
    """RSS é˜…è¯»å™¨æœåŠ¡ (Multi-Agent Version)"""
    
    def __init__(self, config: AppConfig, analysis_mode: str = "deep"):
        self.config = config
        
        # è§£æåˆ†ææ¨¡å¼
        self.analysis_mode = AnalysisMode(analysis_mode)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db = Database(config.storage.database_path)
        self.rss_parser = RSSParser()
        self.content_extractor = ContentExtractor()
        
        # ğŸ†• å¤šæ™ºèƒ½ä½“åˆ†æå™¨
        self.orchestrator = AnalysisOrchestrator(config)
        
        self.email_sender = EmailSender(config.email)
        self.scheduler = Scheduler(config.schedule)
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
        
        logger.info(
            "service_initialized",
            analysis_mode=self.analysis_mode.value,
            vector_store=self.orchestrator.get_stats().get("vector_store", {}),
        )
    
    async def fetch_and_analyze(self):
        """æŠ“å–å¹¶åˆ†ææ–‡ç« """
        logger.info("starting_fetch_cycle", mode=self.analysis_mode.value)
        
        try:
            # 1. æŠ“å– RSS
            articles = await self.rss_parser.fetch_all(self.config.feeds)
            
            if not articles:
                logger.info("no_new_articles")
                return
            
            # 2. è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ç« 
            new_articles = [
                a for a in articles 
                if not self.db.article_exists(a.url)
            ]
            
            if not new_articles:
                logger.info("all_articles_exist", total=len(articles))
                return
            
            logger.info("new_articles_found", count=len(new_articles))
            
            # 3. æå–æ­£æ–‡
            articles_with_content = await self.content_extractor.extract_all(new_articles)
            
            # 4. è½¬æ¢ä¸ºæ–°çš„ Article æ¨¡å‹
            new_format_articles = [
                self._convert_to_new_article(a) for a in articles_with_content
            ]
            
            # 5. ğŸ†• å¤šæ™ºèƒ½ä½“åˆ†æ
            enriched_articles = await self.orchestrator.analyze_batch(
                new_format_articles,
                mode=self.analysis_mode,
                max_concurrent=3,
            )
            
            # 6. ä¿å­˜åˆ°æ•°æ®åº“
            for article in enriched_articles:
                legacy_article = self._convert_to_legacy_article(article)
                self.db.save_analyzed_article(legacy_article)
            
            logger.info("fetch_cycle_complete",
                       fetched=len(articles),
                       new=len(new_articles),
                       analyzed=len(enriched_articles),
                       top_picks=sum(1 for a in enriched_articles if a.is_top_pick))
        
        except Exception as e:
            logger.error("fetch_cycle_failed", error=str(e))
            import traceback
            traceback.print_exc()
    
    async def send_daily_digest(self):
        """å‘é€æ¯æ—¥ç®€æŠ¥ï¼ˆä½¿ç”¨ AI æ™ºèƒ½ç­›é€‰ï¼‰"""
        logger.info("preparing_daily_digest")
        
        try:
            # è·å–æœªå‘é€çš„æ–‡ç« 
            db_articles = self.db.get_unsent_articles(
                limit=self.config.filter.max_articles_per_digest
            )
            
            if not db_articles:
                logger.info("no_articles_to_send")
                return
            
            logger.info("articles_for_curation", count=len(db_articles))
            
            # è½¬æ¢ä¸º EnrichedArticle æ ¼å¼ä¾› Curator ä½¿ç”¨
            enriched_articles = []
            for a in db_articles:
                enriched = EnrichedArticle(
                    url=a.url,
                    title=a.title,
                    content=a.content or "",
                    summary=a.summary or "",
                    source=a.source,
                    category=a.category,
                    overall_score=a.score or 5.0,
                    ai_summary=a.ai_summary or "",
                    is_top_pick=a.is_top_pick,
                    tags=a.tags or [],
                )
                enriched_articles.append(enriched)
            
            # ğŸ†• ä½¿ç”¨ Curator AI æ™ºèƒ½ç­›é€‰
            from src.agents import CuratorAgent
            from src.services.llm import LLMService
            
            curator = CuratorAgent(LLMService(self.config.ai))
            curation_result = await curator.curate(
                enriched_articles,
                max_articles=self.config.filter.max_articles_per_digest,
            )
            
            # æ„å»ºç®€æŠ¥
            top_picks = []
            for article in curation_result["top_picks"]:
                top_picks.append(DigestArticle(
                    title=article.title,
                    url=article.url,
                    source=article.source,
                    category=article.category,
                    score=article.overall_score,
                    summary=article.ai_summary or article.summary,
                    reasoning="",
                    is_top_pick=True,
                    tags=article.tags,
                ))
            
            other_articles = []
            for article in curation_result["quick_reads"]:
                other_articles.append(DigestArticle(
                    title=article.title,
                    url=article.url,
                    source=article.source,
                    category=article.category,
                    score=article.overall_score,
                    summary=article.ai_summary or article.summary,
                    reasoning="",
                    is_top_pick=False,
                    tags=article.tags,
                ))
            
            digest = DailyDigest(
                date=datetime.now(),
                top_picks=top_picks,
                other_articles=other_articles,
                total_fetched=len(db_articles),
                total_analyzed=len(db_articles),
                total_filtered=len(top_picks) + len(other_articles),
            )
            
            logger.info(
                "curation_complete",
                top_picks=len(top_picks),
                quick_reads=len(other_articles),
                excluded=len(curation_result.get("excluded", [])),
                daily_summary=curation_result.get("daily_summary", "")[:100],
            )
            
            # å‘é€é‚®ä»¶
            success = await self.email_sender.send_digest(digest)
            
            if success:
                # åªæ ‡è®°è¢«é€‰ä¸­çš„æ–‡ç« ä¸ºå·²å‘é€
                sent_urls = [a.url for a in top_picks + other_articles]
                self.db.mark_articles_sent(sent_urls)
                logger.info("digest_sent_successfully",
                           top_picks=len(top_picks),
                           other=len(other_articles))
            else:
                logger.error("digest_send_failed")
        
        except Exception as e:
            logger.error("digest_preparation_failed", error=str(e))
            import traceback
            traceback.print_exc()
    
    async def run_once(self, dry_run: bool = False):
        """è¿è¡Œä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await self.fetch_and_analyze()
        
        if not dry_run:
            await self.send_daily_digest()
    
    async def run(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("starting_service", mode=self.analysis_mode.value)
        self._running = True
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        self.scheduler.add_fetch_job(self.fetch_and_analyze)
        self.scheduler.add_digest_job(self.send_daily_digest)
        
        # å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡æŠ“å–
        await self.fetch_and_analyze()
        
        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()
        
        # ä¿æŒè¿è¡Œ
        try:
            while self._running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            pass
        finally:
            self.scheduler.stop()
            self.content_extractor.close()
            logger.info("service_stopped")
    
    def stop(self):
        """åœæ­¢æœåŠ¡"""
        self._running = False
    
    def _convert_to_new_article(self, legacy: LegacyArticle) -> NewArticle:
        """å°†æ—§ç‰ˆ Article è½¬æ¢ä¸ºæ–°ç‰ˆ"""
        return NewArticle(
            url=legacy.url,
            title=legacy.title,
            content=legacy.content,
            summary=legacy.summary,
            source=legacy.source,
            category=legacy.category,
            author=legacy.author,
            published_at=legacy.published_at,
            fetched_at=legacy.fetched_at,
        )
    
    def _convert_to_legacy_article(self, enriched: EnrichedArticle) -> AnalyzedArticle:
        """å°† EnrichedArticle è½¬æ¢ä¸ºæ—§ç‰ˆ AnalyzedArticleï¼ˆç”¨äºæ•°æ®åº“å­˜å‚¨ï¼‰"""
        return AnalyzedArticle(
            url=enriched.url,
            title=enriched.title,
            content=enriched.content,
            summary=enriched.summary,
            source=enriched.source,
            category=enriched.category,
            author=enriched.author,
            published_at=enriched.published_at,
            fetched_at=enriched.fetched_at,
            score=enriched.overall_score,
            ai_summary=enriched.ai_summary,
            is_top_pick=enriched.is_top_pick,
            reasoning=self._build_reasoning(enriched),
            tags=enriched.tags,
        )
    
    def _build_reasoning(self, enriched: EnrichedArticle) -> str:
        """ä» EnrichedArticle æ„å»ºæ¨ç†æ‘˜è¦"""
        parts = []
        
        # å¯ä¿¡åº¦
        if enriched.source_credibility:
            parts.append(f"ä¿¡æº: {enriched.source_credibility.tier}")
        
        # å½±å“
        if enriched.impact_analysis and enriched.impact_analysis.direct_impact:
            parts.append(f"ç›´æ¥å½±å“: {len(enriched.impact_analysis.direct_impact)}é¡¹")
        
        # å¸‚åœºæƒ…ç»ª
        if enriched.market_sentiment:
            parts.append(f"å¸‚åœº: {enriched.market_sentiment.overall}")
        
        # é£é™©
        if enriched.risk_warnings:
            parts.append(f"é£é™©è­¦ç¤º: {len(enriched.risk_warnings)}é¡¹")
        
        return " | ".join(parts) if parts else ""


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="RSS AI Reader - å¤šæ™ºèƒ½ä½“æ™ºèƒ½ RSS é˜…è¯»å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m src.main                         # å¯åŠ¨æœåŠ¡ï¼ˆæ·±åº¦åˆ†ææ¨¡å¼ï¼‰
  python -m src.main --mode quick            # å¿«é€Ÿåˆ†ææ¨¡å¼
  python -m src.main --mode standard         # æ ‡å‡†åˆ†ææ¨¡å¼
  python -m src.main --once                  # è¿è¡Œä¸€æ¬¡
  python -m src.main --once --dry-run        # æµ‹è¯•è¿è¡Œï¼ˆä¸å‘é€é‚®ä»¶ï¼‰
  python -m src.main --test-email            # å‘é€æµ‹è¯•é‚®ä»¶
        """
    )
    
    parser.add_argument(
        "--mode", "-m",
        type=str,
        choices=["quick", "standard", "deep"],
        default="deep",
        help="åˆ†ææ¨¡å¼: quick(å¿«é€Ÿ), standard(æ ‡å‡†), deep(æ·±åº¦)"
    )
    
    parser.add_argument(
        "--once", "-1",
        action="store_true",
        help="åªè¿è¡Œä¸€æ¬¡ï¼Œç„¶åé€€å‡º"
    )
    
    parser.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼Œä¸å‘é€é‚®ä»¶"
    )
    
    parser.add_argument(
        "--test-email", "-t",
        action="store_true",
        help="å‘é€æµ‹è¯•é‚®ä»¶"
    )
    
    parser.add_argument(
        "--config-dir", "-c",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶ç›®å½•"
    )
    
    return parser.parse_args()


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    if args.config_dir:
        config = reload_config(Path(args.config_dir))
    else:
        config = get_config()
    
    # åˆ›å»ºæœåŠ¡
    service = RSSReaderService(config, analysis_mode=args.mode)
    
    # å¤„ç†ä¿¡å·
    loop = asyncio.get_event_loop()
    
    def signal_handler():
        logger.info("received_shutdown_signal")
        service.stop()
    
    for sig in (signal.SIGTERM, signal.SIGINT):
        loop.add_signal_handler(sig, signal_handler)
    
    # è¿è¡Œæ¨¡å¼
    if args.test_email:
        # æµ‹è¯•é‚®ä»¶
        email_sender = EmailSender(config.email)
        success = await email_sender.send_test_email()
        if success:
            print("âœ… æµ‹è¯•é‚®ä»¶å‘é€æˆåŠŸï¼")
        else:
            print("âŒ æµ‹è¯•é‚®ä»¶å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            sys.exit(1)
    
    elif args.once:
        # è¿è¡Œä¸€æ¬¡
        mode_names = {"quick": "å¿«é€Ÿ", "standard": "æ ‡å‡†", "deep": "æ·±åº¦"}
        print(f"ğŸ” ä½¿ç”¨ {mode_names[args.mode]} åˆ†ææ¨¡å¼...")
        await service.run_once(dry_run=args.dry_run)
        print("âœ… è¿è¡Œå®Œæˆï¼")
    
    else:
        # æŒç»­è¿è¡Œ
        mode_names = {"quick": "å¿«é€Ÿ", "standard": "æ ‡å‡†", "deep": "æ·±åº¦"}
        print("ğŸš€ RSS AI Reader æœåŠ¡å·²å¯åŠ¨ï¼ˆå¤šæ™ºèƒ½ä½“ç‰ˆæœ¬ï¼‰")
        print(f"ğŸ§  åˆ†ææ¨¡å¼: {mode_names[args.mode]}")
        print(f"ğŸ“¥ æŠ“å–é—´éš”: {config.schedule.fetch_interval}")
        digest_times_str = "ã€".join(config.schedule.digest_times)
        print(f"ğŸ“§ ç®€æŠ¥æ—¶é—´: æ¯å¤© {digest_times_str}")
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡...")
        await service.run()


def main():
    """ä¸»å…¥å£"""
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()
