"""Analysis Orchestrator - åè°ƒæ‰€æœ‰ Agent çš„å·¥ä½œæµ"""

import asyncio
import time
from typing import Optional
import structlog

from ..config import AppConfig
from ..models.article import Article, EnrichedArticle
from ..models.agent import AgentContext, AgentOutput, AgentTrace, AnalysisMode
from ..models.information import InformationUnit
from ..models.entity import ExtractedEntity, ExtractedRelation
from ..services.llm import LLMService
from ..storage.vector_store import VectorStore
from ..storage.information_store import InformationStore
from ..storage.entity_store import EntityStore

from .collector import CollectorAgent
from .librarian import LibrarianAgent
from .editor import EditorAgent
from .extractor import InformationExtractorAgent
from .merger import InformationMergerAgent
from .analysts import SkepticAnalyst, EconomistAnalyst, DetectiveAnalyst
from .trace_manager import TraceManager

logger = structlog.get_logger()


class AnalysisOrchestrator:
    """
    åˆ†æåè°ƒå™¨
    
    ç®¡ç†å¤šæ™ºèƒ½ä½“å·¥ä½œæµï¼š
    Article â†’ Collector â†’ Librarian â†’ Analysts (å¹¶è¡Œ) â†’ Editor â†’ EnrichedArticle
    
    æ”¯æŒä¸‰ç§åˆ†ææ¨¡å¼ï¼š
    - QUICK: ä»… Collector (å¿«é€Ÿè¯„åˆ†å’Œæ‘˜è¦)
    - STANDARD: Collector + Librarian + Editor (åŸºç¡€åˆ†æ)
    - DEEP: å®Œæ•´æµç¨‹ (æ‰€æœ‰ Agent)
    """
    
    def __init__(self, config: AppConfig, enable_trace: bool = True, progress_tracker=None):
        self.config = config
        self.llm_service = LLMService(config.ai)
        self.enable_trace = enable_trace
        self.progress_tracker = progress_tracker  # ğŸ†• å¯é€‰çš„è¿›åº¦è¿½è¸ªå™¨
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store_path = str(config.storage.database_path).replace(".db", "_vectors")
        self.vector_store = VectorStore(vector_store_path)
        
        # åˆå§‹åŒ–è¿½è¸ªç®¡ç†å™¨
        trace_dir = str(config.storage.database_path).replace(".db", "_traces")
        self.trace_manager = TraceManager(trace_dir) if enable_trace else None
        
        # åˆå§‹åŒ–æ‰€æœ‰ Agent
        self.collector = CollectorAgent(self.llm_service)
        self.librarian = LibrarianAgent(self.llm_service, self.vector_store)
        self.editor = EditorAgent(self.llm_service)
        
        self.analysts = {
            "skeptic": SkepticAnalyst(self.llm_service),
            "economist": EconomistAnalyst(self.llm_service),
            "detective": DetectiveAnalyst(self.llm_service),
        }
        
        # æ–°æ¶æ„ç»„ä»¶
        self.info_store: Optional[InformationStore] = None
        self.entity_store: Optional[EntityStore] = None  # ğŸ†• çŸ¥è¯†å›¾è°±å­˜å‚¨
        self.extractor = InformationExtractorAgent(self.llm_service)
        self.merger = InformationMergerAgent(self.llm_service)

    def set_information_store(self, store: InformationStore):
        """æ³¨å…¥ InformationStore"""
        self.info_store = store
    
    def set_entity_store(self, store: EntityStore):
        """æ³¨å…¥ EntityStore (çŸ¥è¯†å›¾è°±)"""
        self.entity_store = store
    
    async def analyze_article(
        self, 
        article: Article,
        mode: AnalysisMode = AnalysisMode.DEEP,
    ) -> EnrichedArticle:
        """
        åˆ†æå•ç¯‡æ–‡ç« 
        
        Args:
            article: å¾…åˆ†æçš„æ–‡ç« 
            mode: åˆ†ææ¨¡å¼ (QUICK, STANDARD, DEEP)
            
        Returns:
            EnrichedArticle åŒ…å«å®Œæ•´åˆ†æç»“æœ
        """
        start_time = time.time()
        logger.info(
            "analysis_started",
            title=article.title[:50],
            mode=mode.value,
        )
        
        # å¼€å§‹è¿½è¸ªä¼šè¯
        if self.trace_manager:
            self.trace_manager.start_session(article.url, article.title)
        
        context = AgentContext(
            original_article=article,
            analysis_mode=mode,
        )
        
        try:
            if mode == AnalysisMode.QUICK:
                enriched = await self._quick_analysis(article, context)
            elif mode == AnalysisMode.STANDARD:
                enriched = await self._standard_analysis(article, context)
            else:  # DEEP
                enriched = await self._deep_analysis(article, context)
            
            duration = time.time() - start_time
            logger.info(
                "analysis_completed",
                title=article.title[:50],
                mode=mode.value,
                score=enriched.overall_score,
                is_top_pick=enriched.is_top_pick,
                duration=f"{duration:.2f}s",
                total_tokens=context.get_total_tokens(),
            )
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            if self.trace_manager:
                self.trace_manager.save_final_result(enriched)
                session_path = self.trace_manager.end_session()
                logger.info("trace_saved", path=str(session_path))
            
            return enriched
            
        except Exception as e:
            logger.error(
                "analysis_failed",
                title=article.title[:50],
                error=str(e),
            )
            if self.trace_manager:
                self.trace_manager.end_session()
            # è¿”å›åŸºç¡€çš„ EnrichedArticle
            return EnrichedArticle.from_article(article)
    
    async def analyze_batch(
        self,
        articles: list[Article],
        mode: AnalysisMode = AnalysisMode.DEEP,
        max_concurrent: int = 3,
    ) -> list[EnrichedArticle]:
        """
        æ‰¹é‡åˆ†ææ–‡ç« 
        
        Args:
            articles: å¾…åˆ†æçš„æ–‡ç« åˆ—è¡¨
            mode: åˆ†ææ¨¡å¼
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            EnrichedArticle åˆ—è¡¨
        """
        if not articles:
            return []
        
        logger.info(
            "batch_analysis_started",
            count=len(articles),
            mode=mode.value,
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def analyze_with_semaphore(article: Article) -> EnrichedArticle:
            async with semaphore:
                return await self.analyze_article(article, mode)
        
        tasks = [analyze_with_semaphore(a) for a in articles]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        enriched_articles = []
        for article, result in zip(articles, results):
            if isinstance(result, Exception):
                logger.error(
                    "article_analysis_failed",
                    title=article.title[:50],
                    error=str(result),
                )
                enriched_articles.append(EnrichedArticle.from_article(article))
            else:
                enriched_articles.append(result)
        
        # æŒ‰è¯„åˆ†æ’åº
        enriched_articles.sort(key=lambda x: x.overall_score, reverse=True)
        
        logger.info(
            "batch_analysis_completed",
            count=len(enriched_articles),
            top_picks=sum(1 for a in enriched_articles if a.is_top_pick),
        )
        
        return enriched_articles
    
    async def _quick_analysis(
        self, 
        article: Article, 
        context: AgentContext
    ) -> EnrichedArticle:
        """å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šä»… Collector"""
        # Step 1: Collector
        collector_result = await self.collector.safe_process(article, context)
        context.add_trace(collector_result.trace)
        self._save_trace("Collector", article, collector_result)
        
        # ç›´æ¥æ„å»ºç»“æœ
        extracted = context.extracted_5w1h or {}
        
        return EnrichedArticle(
            url=article.url,
            title=article.title,
            content=article.content,
            summary=article.summary,
            source=article.source,
            category=article.category,
            author=article.author,
            published_at=article.published_at,
            fetched_at=article.fetched_at,
            who=extracted.get("who", []),
            what=extracted.get("what", ""),
            when=extracted.get("when", ""),
            where=extracted.get("where", ""),
            why=extracted.get("why", ""),
            how=extracted.get("how", ""),
            ai_summary=extracted.get("core_summary", ""),
            tags=extracted.get("tags", []),
            overall_score=5.0,  # å¿«é€Ÿæ¨¡å¼é»˜è®¤ä¸­ç­‰åˆ†æ•°
            analysis_mode=AnalysisMode.QUICK.value,
            agent_traces=context.traces,
        )
    
    async def _standard_analysis(
        self, 
        article: Article, 
        context: AgentContext
    ) -> EnrichedArticle:
        """æ ‡å‡†åˆ†ææ¨¡å¼ï¼šCollector + Librarian + Editor"""
        # Step 1: Collector
        collector_result = await self.collector.safe_process(article, context)
        context.add_trace(collector_result.trace)
        self._save_trace("Collector", article, collector_result)
        
        # Step 2: Librarian
        librarian_result = await self.librarian.safe_process(article, context)
        context.add_trace(librarian_result.trace)
        self._save_trace("Librarian", article, librarian_result)
        
        # Step 3: ç®€åŒ–çš„ Editorï¼ˆæ²¡æœ‰åˆ†æå¸ˆæŠ¥å‘Šï¼‰
        editor_result = await self.editor.process(
            article=article,
            context=context,
            analyst_reports={},
        )
        context.add_trace(editor_result.trace)
        self._save_trace("Editor", article, editor_result)
        
        enriched = editor_result.data
        enriched.analysis_mode = AnalysisMode.STANDARD.value
        enriched.agent_traces = context.traces
        
        return enriched
    
    async def _deep_analysis(
        self, 
        article: Article, 
        context: AgentContext
    ) -> EnrichedArticle:
        """æ·±åº¦åˆ†ææ¨¡å¼ï¼šå®Œæ•´æµç¨‹"""
        # Step 1: Collector
        collector_result = await self.collector.safe_process(article, context)
        context.add_trace(collector_result.trace)
        self._save_trace("Collector", article, collector_result)
        
        # Step 2: Librarian
        librarian_result = await self.librarian.safe_process(article, context)
        context.add_trace(librarian_result.trace)
        self._save_trace("Librarian", article, librarian_result)
        
        # Step 3: Analyst Team (å¹¶è¡Œæ‰§è¡Œ)
        analyst_tasks = {
            name: analyst.safe_process(article, context)
            for name, analyst in self.analysts.items()
        }
        
        analyst_results = {}
        for name, task in analyst_tasks.items():
            result = await task
            analyst_results[name] = result.data
            context.add_trace(result.trace)
            context.analyst_reports[name] = result.data
            self._save_trace(f"Analyst_{name}", article, result)
        
        # Step 4: Editor
        editor_result = await self.editor.process(
            article=article,
            context=context,
            analyst_reports=analyst_results,
        )
        context.add_trace(editor_result.trace)
        self._save_trace("Editor", article, editor_result)
        
        enriched = editor_result.data
        enriched.analysis_mode = AnalysisMode.DEEP.value
        enriched.agent_traces = context.traces
        
        return enriched
    
    def _save_trace(self, agent_name: str, article: Article, result: AgentOutput):
        """ä¿å­˜ Agent è¿½è¸ªæ•°æ®"""
        if not self.trace_manager:
            return
        
        trace = result.trace
        if trace:
            self.trace_manager.save_agent_output(
                agent_name=agent_name,
                input_data={"title": article.title, "url": article.url},
                output_data=result.data,
                duration_seconds=trace.duration_seconds,
                token_usage=trace.token_usage,
                error=trace.error,
            )
    
    async def process_article_information_centric(self, article: Article) -> list[InformationUnit]:
        """
        ä»¥ä¿¡æ¯ä¸ºä¸­å¿ƒçš„å¤„ç†æµç¨‹
        1. Extract: æ–‡ç«  -> ä¿¡æ¯å•å…ƒåˆ—è¡¨
        2. Merge: ä¸åº“ä¸­ç°æœ‰å•å…ƒå»é‡/åˆå¹¶ (ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦)
        3. Save: æŒä¹…åŒ–
        """
        if not self.info_store:
            logger.warning("information_store_not_configured")
            return []
            
        logger.info("processing_info_centric", title=article.title)
        
        # Context
        context = AgentContext(
            original_article=article,
            analysis_mode=AnalysisMode.DEEP  # Default to DEEP for info centric to get best results
        )
        if self.trace_manager:
            self.trace_manager.start_session(article.url, article.title + " [INFO_FLOW]")

        try:
            # ğŸ†• 0. OPTIONAL: Run Analysts First (Consultant Mode)
            # This provides high-quality perspective to the extractor
            if context.analysis_mode == AnalysisMode.DEEP:
                logger.info("running_consultant_analysts")
                
                # Define which analysts to run
                analyst_names = ["skeptic", "economist", "detective"]
                
                # Run them in parallel using asyncio.gather for true concurrency
                tasks = []
                names = []
                
                for name in analyst_names:
                    if name in self.analysts:
                        tasks.append(self.analysts[name].safe_process(article, context))
                        names.append(name)
                
                if tasks:
                    results = await asyncio.gather(*tasks)
                    
                    analyst_results = {}
                    for name, result in zip(names, results):
                        analyst_results[name] = result.data
                        context.add_trace(result.trace)
                        # Save trace
                        self._save_trace(f"Consultant_{name}", article, result)
                    
                    # Store reports in context for Extractor to see
                    context.analyst_reports = analyst_results
                    logger.info("consultant_phase_complete", reports=list(analyst_results.keys()))

            # 1. Extract (Augmented by Analyst Reports if available)
            units = await self.extractor.extract(article, context)
            logger.info("extracted_units", count=len(units))
            
            final_units = []
            
            for unit in units:
                # 2. Check & Merge using Semantic Similarity (Primary) + Fingerprint (Fallback)
                
                # 2.1 é¦–å…ˆæ£€æŸ¥ç²¾ç¡®æŒ‡çº¹åŒ¹é…
                existing = self.info_store.get_unit_by_fingerprint(unit.fingerprint)
                
                if existing:
                    logger.info("merging_exact_fingerprint_match", fingerprint=unit.fingerprint)
                    merged = await self.merger.merge([existing, unit])
                    await self.info_store.save_unit(merged)
                    final_units.append(merged)
                    
                    if self.trace_manager:
                        self.trace_manager.save_agent_output(
                            agent_name="Merger",
                            input_data={"unit_new": unit.title, "unit_exist": existing.title, "match_type": "fingerprint"},
                            output_data={"merged": merged.title},
                            duration_seconds=0, token_usage={}
                        )
                    continue
                
                # 2.2 å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢
                similar_units = await self.info_store.find_similar_units(unit, threshold=0.6, top_k=3)
                
                if similar_units:
                    # æ‰¾åˆ°è¯­ä¹‰ç›¸ä¼¼çš„å•å…ƒï¼Œè¿›è¡Œåˆå¹¶
                    logger.info(
                        "merging_semantically_similar_units",
                        new_title=unit.title,
                        similar_count=len(similar_units),
                        similar_titles=[u.title for u in similar_units]
                    )
                    
                    # åˆå¹¶æ‰€æœ‰ç›¸ä¼¼å•å…ƒ + æ–°å•å…ƒ
                    all_to_merge = similar_units + [unit]
                    merged = await self.merger.merge(all_to_merge)
                    
                    # æ›´æ–°åˆå¹¶åçš„å•å…ƒï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªç›¸ä¼¼å•å…ƒçš„ ID ä½œä¸ºä¸» IDï¼‰
                    merged.id = similar_units[0].id
                    merged.fingerprint = similar_units[0].fingerprint
                    
                    # ğŸ†• ä¿ç•™åŸæœ‰çš„åˆ†æå†…å®¹ï¼ˆå¦‚æœæœ‰ä»·å€¼ï¼‰
                    # å¦‚æœæ–°å•å…ƒæœ‰åˆ†æå¸ˆæ”¯æŒï¼Œå¯èƒ½æ¯”æ—§çš„æ›´å¥½ï¼ŒMerger éœ€è¦æ™ºèƒ½åˆ¤æ–­
                    # è¿™é‡Œå‡è®¾ Merger å·²ç»èƒ½å¤Ÿå¤„ç†å¥½
                    
                    await self.info_store.save_unit(merged)
                    final_units.append(merged)
                    
                    if self.trace_manager:
                        self.trace_manager.save_agent_output(
                            agent_name="Merger",
                            input_data={
                                "unit_new": unit.title, 
                                "similar_units": [u.title for u in similar_units],
                                "match_type": "semantic"
                            },
                            output_data={"merged": merged.title, "source_count": merged.source_count},
                            duration_seconds=0, token_usage={}
                        )
                else:
                    # å®Œå…¨æ–°çš„å•å…ƒ
                    await self.info_store.save_unit(unit)
                    final_units.append(unit)
                
                # ğŸ†• å¤„ç†å®ä½“å’Œå…³ç³» (æ„å»ºçŸ¥è¯†å›¾è°±)
                if self.entity_store and (unit.extracted_entities or unit.extracted_relations):
                    try:
                        # è½¬æ¢ä¸º ExtractedEntity å¯¹è±¡
                        extracted_entities = [
                            ExtractedEntity(
                                name=e.get("name", ""),
                                aliases=e.get("aliases", []),
                                type=e.get("type", "COMPANY"),
                                role=e.get("role", "ä¸»è§’"),
                                state_change=e.get("state_change"),
                            ) for e in unit.extracted_entities if isinstance(e, dict) and e.get("name")
                        ]
                        
                        extracted_relations = [
                            ExtractedRelation(
                                source=r.get("source", ""),
                                target=r.get("target", ""),
                                relation=r.get("relation", "peer"),
                                evidence=r.get("evidence", ""),
                            ) for r in unit.extracted_relations if isinstance(r, dict) and r.get("source")
                        ]
                        
                        if extracted_entities:
                            entity_id_map = self.entity_store.process_extracted_entities(
                                unit_id=unit.id,
                                entities=extracted_entities,
                                relations=extracted_relations,
                                event_time=unit.event_time,
                            )
                            logger.debug("entities_processed", 
                                        unit_id=unit.id, 
                                        entity_count=len(entity_id_map))
                    except Exception as e:
                        logger.warning("entity_processing_failed", unit_id=unit.id, error=str(e))
            
            if self.trace_manager:
                self.trace_manager.end_session()
                
            return final_units
            
        except Exception as e:
            logger.error("info_flow_failed", error=str(e))
            if self.trace_manager:
                self.trace_manager.end_session()
            return []

    def get_stats(self) -> dict:
        """è·å–åè°ƒå™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "vector_store": self.vector_store.get_stats() if self.vector_store else {},
            "trace_enabled": self.trace_manager is not None,
            "info_store_enabled": self.info_store is not None,
            "agents": [
                self.collector.name,
                self.librarian.name,
                self.editor.name,
                self.extractor.name,
                self.merger.name,
            ] + list(self.analysts.keys()),
        }

