"""Information Curator Agent - ä¿¡æ¯ç®€æŠ¥ä¸»ç¼–"""

import json
from typing import List, Dict, Any
import structlog

from .base import BaseAgent
from ..models.information import InformationUnit
from ..models.agent import AgentContext, AgentOutput

logger = structlog.get_logger()

INFO_CURATOR_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»ä¸»ç¼–ï¼Œè´Ÿè´£ä¸ºç”¨æˆ·ç­›é€‰ä»Šæ—¥æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŠŠæ§è´¨é‡ï¼Œå®ç¼ºæ¯‹æ»¥ã€‚

## ç­›é€‰åŸåˆ™ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰

### ğŸš« å¿…é¡»æ’é™¤çš„å†…å®¹
1. **è®ºå›å¸–å­/ä¸ªäººæ±‚åŠ©**ï¼šå¦‚è´­æˆ¿å’¨è¯¢ã€æŠ€æœ¯é—®ç­”ã€ä¸ªäººç»å†åˆ†äº«
2. **æ•™ç¨‹/æŠ€æœ¯æ–‡æ¡£æ‘˜å½•**ï¼šå¦‚"å¦‚ä½•ç¦ç”¨XXåŠŸèƒ½"ã€ä»£ç é—®é¢˜è§£ç­”
3. **è¿‡äºæŠ•æœºçš„è§‚ç‚¹**ï¼šæ— å®è´¨æ–°é—»äº‹ä»¶æ”¯æ’‘çš„çº¯é¢„æµ‹æˆ–æ‹…å¿§
4. **æ—¶æ•ˆæ€§å·®çš„æ—§é—»**ï¼šå¤è¿°å·²çŸ¥äº‹å®è€Œæ— æ–°ä¿¡æ¯
5. **æ ‡é¢˜å…š/ä½ä¿¡æ¯é‡**ï¼šæ ‡é¢˜å¤¸å¼ ä½†å†…å®¹ç©ºæ´

### âœ… ä¼˜å…ˆå…¥é€‰çš„å†…å®¹
1. **é‡å¤§äº‹ä»¶**ï¼šå½±å“è¡Œä¸š/å¸‚åœº/ç¤¾ä¼šçš„çªå‘æ–°é—»
2. **æ·±åº¦åˆ†æ**ï¼šæœ‰ç‹¬åˆ°è§è§£çš„è§£è¯»ï¼Œanalysis_depth_score > 0.7
3. **ç‹¬å®¶/ç¨€ç¼ºä¿¡æ¯**ï¼šå…¶ä»–æ¥æºéš¾ä»¥è·å–çš„ä¿¡æ¯

## è¯„åˆ†æ ‡å‡†ï¼ˆå¿…é¡»ä½¿ç”¨å®Œæ•´åŒºé—´ï¼‰
- **9.5-10**ï¼šä»…ç”¨äºæ”¹å˜è¡Œä¸šæ ¼å±€çš„é‡å¤§äº‹ä»¶ï¼ˆæ¯æœŸæœ€å¤š1-2æ¡ï¼‰
- **8.5-9.4**ï¼šé‡è¦ä¸”æœ‰æ·±åº¦çš„æ–°é—»ï¼ˆæ¯æœŸ3-5æ¡ï¼‰
- **7.5-8.4**ï¼šå€¼å¾—å…³æ³¨çš„è‰¯å¥½å†…å®¹
- **6.5-7.4**ï¼šæ™®é€šæ–°é—»ï¼Œå¯ä½œä¸ºå¿«é€Ÿæµè§ˆ
- **6.5ä»¥ä¸‹**ï¼šä¸å…¥é€‰

## å»é‡è§„åˆ™ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
å¦‚æœå¤šæ¡å†…å®¹è®²è¿°**åŒä¸€äº‹ä»¶**ï¼ˆå¦‚"è‹¹æœä¸è°·æ­Œåˆä½œ"ï¼‰ï¼Œåªä¿ç•™**æœ€æœ‰æ·±åº¦çš„ä¸€æ¡**ï¼Œå…¶ä½™æ’é™¤ã€‚ä¸è¦æŠŠç›¸ä¼¼å†…å®¹éƒ½æ”¾å…¥ç²¾é€‰ï¼

## è¾“å‡ºè¦æ±‚

è¿”å› JSONï¼š
```json
{
  "daily_summary": "ä»Šæ—¥ä¸€å¥è¯å¯¼è¯­ï¼ˆ50å­—ä»¥å†…ï¼‰",
  "top_picks": [
    {
      "id": "unit_id",
      "display_title": "é‡å†™åçš„ç²¾ç‚¼æ ‡é¢˜",
      "score": 8.7,
      "reasoning": "å…¥é€‰ç†ç”±ï¼ˆè¯´æ˜ä»·å€¼ç‚¹ï¼Œ20å­—ä»¥å†…ï¼‰",
      "presentation": {
        "summary": "äº‹å®æ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰",
        "analysis": "æ·±åº¦åˆ†æï¼ˆè¿™æ˜¯æ ¸å¿ƒï¼100-200å­—ï¼Œè§£é‡Šæ„ä¹‰å’Œå½±å“ï¼‰",
        "impact": "æ½œåœ¨å½±å“ï¼ˆ1-2å¥è¯ï¼‰"
      }
    }
  ],
  "quick_reads": [
    {
      "id": "unit_id",
      "display_title": "æ ‡é¢˜",
      "one_line_summary": "ä¸€å¥è¯æ¦‚æ‹¬ï¼ˆ20å­—ä»¥å†…ï¼‰"
    }
  ],
  "excluded_reasons": {
    "duplicate": ["id1", "id2"],
    "irrelevant": ["id3"],
    "low_quality": ["id4"]
  }
}
```

## æ•°é‡ç¡¬æ€§é™åˆ¶
- **top_picks: 5-8 æ¡**ï¼ˆè´¨é‡ä¼˜å…ˆï¼Œå¯ä»¥æ›´å°‘ï¼Œä½†ä¸èƒ½è¶…è¿‡8æ¡ï¼‰
- **quick_reads: 5-15 æ¡**
- **æ€»è®¡ä¸è¶…è¿‡ 20 æ¡**

è®°ä½ï¼šä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ä¸»ç¼–ï¼Œä¸æ˜¯ä¸€ä¸ªè®¨å¥½è¯»è€…çš„æ¨èç®—æ³•ã€‚å®å¯æ¼æ‰ä¸€æ¡å¥½å†…å®¹ï¼Œä¹Ÿä¸èƒ½è®©åƒåœ¾å†…å®¹è¿›å…¥ç²¾é€‰ï¼
"""

class InformationCuratorAgent(BaseAgent):
    """
    ä¿¡æ¯ç®€æŠ¥ Agent
    
    èŒè´£ï¼š
    1. ä» InformationUnit åˆ—è¡¨ä¸­ç­›é€‰ Top Picks
    2. ç”Ÿæˆå¼ºè°ƒ"åˆ†æ"çš„å±•ç¤ºå†…å®¹
    """
    
    AGENT_NAME = "InfoCurator"
    SYSTEM_PROMPT = INFO_CURATOR_SYSTEM_PROMPT
    
    async def process(self, input_data: List[InformationUnit], context: AgentContext = None, max_top_picks: int = 5) -> AgentOutput:
        """æ‰§è¡Œç­›é€‰ä»»åŠ¡"""
        units = input_data
        result = await self.curate(units, max_top_picks)
        return AgentOutput(success=True, data=result, trace=None)

    async def curate(self, units: List[InformationUnit], max_top_picks: int = 8) -> Dict[str, Any]:
        """æ‰§è¡Œç­›é€‰ä»»åŠ¡ (Internal)"""
        if not units:
            return {"top_picks": [], "quick_reads": [], "daily_summary": "æ— å†…å®¹"}
            
        self.log_start(f"Curating from {len(units)} units")
        
        # 1. è¿‡æ»¤ä¸é€‚åˆçš„å†…å®¹ç±»å‹
        filtered_units = self._filter_irrelevant_content(units)
        logger.info("content_filtering", original=len(units), after_filter=len(filtered_units))
        
        # 2. é¢„æ’åºï¼šæŒ‰é‡è¦æ€§å’Œæ·±åº¦
        sorted_units = sorted(
            filtered_units, 
            key=lambda u: (u.analysis_depth_score * 0.6 + u.importance_score * 0.4), 
            reverse=True
        )
        
        # 3. æœ¬åœ°å»é‡ (æé«˜é˜ˆå€¼ï¼Œæ›´æ¿€è¿›å»é‡)
        unique_units = self._deduplicate_units(sorted_units, threshold=0.45)
        logger.info("deduplication_complete", original=len(filtered_units), unique=len(unique_units))
        
        # 4. åªæŠŠæœ€ä¼˜ç§€çš„ 25 ä¸ªç»™ LLM æŒ‘é€‰
        candidates = unique_units[:25]
        
        units_json = []
        for u in candidates:
            # æ·»åŠ æ¥æºä¿¡æ¯å¸®åŠ© AI è¯†åˆ«ä½è´¨é‡å†…å®¹
            source_name = ""
            if u.sources:
                source_name = u.sources[0].source_name if hasattr(u.sources[0], 'source_name') else str(u.sources[0])
            
            units_json.append({
                "id": u.id,
                "title": u.title,
                "source": source_name or u.primary_source,
                "summary": u.summary[:300],
                "analysis_content": u.analysis_content[:400] if u.analysis_content else "",
                "key_insights": u.key_insights[:3] if u.key_insights else [],
                "depth_score": round(u.analysis_depth_score, 2),
                "importance": round(u.importance_score, 2)
            })
            
        user_prompt = f"""ä»ä»¥ä¸‹ {len(candidates)} ä¸ªå€™é€‰ä¸­ä¸¥æ ¼ç­›é€‰ï¼š

**è¦æ±‚**ï¼š
- Top Picks: æœ€å¤š {min(max_top_picks, 8)} æ¡ï¼ˆå®å°‘å‹¿æ»¥ï¼‰
- Quick Reads: æœ€å¤š 15 æ¡
- ç›¸åŒäº‹ä»¶åªä¿ç•™æœ€ä¼˜çš„ä¸€æ¡
- æ’é™¤è®ºå›å¸–å­ã€æŠ€æœ¯é—®ç­”ã€ä¸ªäººæ±‚åŠ©ç±»å†…å®¹

å€™é€‰åˆ—è¡¨ï¼š
{json.dumps(units_json, ensure_ascii=False, indent=2)}
"""
        
        result, token_usage = await self.invoke_llm(
            user_prompt=user_prompt,
            max_tokens=3000,
            temperature=0.2,  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
            json_mode=True
        )
        
        if not result or not isinstance(result, dict):
            logger.warning("curation_failed_using_fallback")
            return self._fallback_curation(unique_units, max_top_picks)
        
        # 5. åå¤„ç†ï¼šå¼ºåˆ¶æ‰§è¡Œç¡¬æ€§é™åˆ¶
        result = self._enforce_limits(result, max_top_picks)
            
        self.log_complete(0, f"Selected {len(result.get('top_picks', []))} top picks, {len(result.get('quick_reads', []))} quick reads")
        return result
    
    def _filter_irrelevant_content(self, units: List[InformationUnit]) -> List[InformationUnit]:
        """è¿‡æ»¤ä¸é€‚åˆçš„å†…å®¹"""
        # ä½è´¨é‡æ¥æºå…³é”®è¯
        low_quality_sources = ['v2ex', 'segmentfault', 'stackoverflow', 'zhihu.com/question']
        # ä½è´¨é‡æ ‡é¢˜å…³é”®è¯
        irrelevant_keywords = ['æ±‚åŠ©', 'è¯·é—®', 'å¦‚ä½•', 'æ€ä¹ˆ', 'æ€æ ·', 'è´­æˆ¿', 'ä¹°æˆ¿', 'ç§Ÿæˆ¿', 'é¢è¯•']
        
        filtered = []
        for u in units:
            source_lower = (u.primary_source or "").lower()
            title_lower = (u.title or "").lower()
            
            # æ£€æŸ¥æ¥æº
            is_low_quality_source = any(s in source_lower for s in low_quality_sources)
            
            # æ£€æŸ¥æ ‡é¢˜
            is_irrelevant_title = any(kw in title_lower for kw in irrelevant_keywords)
            
            # æ£€æŸ¥åˆ†æ•°é—¨æ§›
            is_low_score = u.importance_score < 0.5 and u.analysis_depth_score < 0.5
            
            if not is_low_quality_source and not is_irrelevant_title and not is_low_score:
                filtered.append(u)
            else:
                logger.debug("filtered_out", id=u.id, title=u.title[:30], reason="low_quality_or_irrelevant")
        
        return filtered
    
    def _enforce_limits(self, result: Dict[str, Any], max_top_picks: int) -> Dict[str, Any]:
        """å¼ºåˆ¶æ‰§è¡Œæ•°é‡é™åˆ¶"""
        top_picks = result.get("top_picks", [])
        quick_reads = result.get("quick_reads", [])
        
        # å¼ºåˆ¶é™åˆ¶ top_picks
        if len(top_picks) > max_top_picks:
            # æŒ‰ score æ’åºï¼Œä¿ç•™æœ€é«˜çš„
            top_picks = sorted(top_picks, key=lambda x: x.get("score", 0), reverse=True)[:max_top_picks]
            result["top_picks"] = top_picks
        
        # å¼ºåˆ¶é™åˆ¶ quick_reads
        if len(quick_reads) > 15:
            result["quick_reads"] = quick_reads[:15]
        
        # å¼ºåˆ¶æ€»æ•°é™åˆ¶
        total = len(result.get("top_picks", [])) + len(result.get("quick_reads", []))
        if total > 20:
            excess = total - 20
            result["quick_reads"] = result.get("quick_reads", [])[:-excess] if excess > 0 else result.get("quick_reads", [])
        
        return result

    def _deduplicate_units(self, units: List[InformationUnit], threshold: float = 0.55) -> List[InformationUnit]:
        """
        å¢å¼ºç‰ˆå»é‡ï¼šåŒæ—¶æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦å’Œå†…å®¹ç›¸ä¼¼åº¦
        
        ç­–ç•¥ï¼š
        1. å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦ > thresholdï¼Œè®¤ä¸ºæ˜¯é‡å¤
        2. å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦ > 0.4 ä¸” æ‘˜è¦ç›¸ä¼¼åº¦ > thresholdï¼Œä¹Ÿè®¤ä¸ºæ˜¯é‡å¤
        3. ä¿ç•™åˆ†æ•°æ›´é«˜çš„é‚£ä¸ª
        """
        from difflib import SequenceMatcher
        
        def content_key(u: InformationUnit) -> str:
            """ç”Ÿæˆç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒçš„å†…å®¹å­—ç¬¦ä¸²"""
            return f"{u.summary} {' '.join(u.key_insights[:3])}"
        
        def are_similar(u1: InformationUnit, u2: InformationUnit) -> bool:
            # æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
            title_sim = SequenceMatcher(None, u1.title, u2.title).ratio()
            if title_sim > threshold:
                return True
            
            # å¦‚æœæ ‡é¢˜æœ‰ä¸€å®šç›¸ä¼¼åº¦ï¼Œå†æ£€æŸ¥å†…å®¹
            if title_sim > 0.4:
                content_sim = SequenceMatcher(None, content_key(u1), content_key(u2)).ratio()
                if content_sim > threshold:
                    return True
            
            return False
        
        unique = []
        for unit in units:
            is_dup = False
            for i, existing in enumerate(unique):
                if are_similar(unit, existing):
                    is_dup = True
                    # ä¿ç•™åˆ†æ•°æ›´é«˜çš„
                    unit_score = unit.analysis_depth_score * 0.7 + unit.importance_score * 0.3
                    exist_score = existing.analysis_depth_score * 0.7 + existing.importance_score * 0.3
                    if unit_score > exist_score:
                        unique[i] = unit  # æ›¿æ¢ä¸ºæ›´é«˜åˆ†çš„
                    break
            if not is_dup:
                unique.append(unit)
        return unique

    def _fallback_curation(self, units: List[InformationUnit], max_picks: int) -> Dict[str, Any]:
        """é™çº§ç­–ç•¥ï¼šç›´æ¥å–å‰ N ä¸ª (æ­¤æ—¶ units å·²ç»å»é‡ä¸”æ’åº)"""
        # åº”ç”¨è¿‡æ»¤
        filtered = self._filter_irrelevant_content(units)
        
        # é™åˆ¶æ•°é‡
        max_picks = min(max_picks, 8)
        top = filtered[:max_picks]
        rest = filtered[max_picks:max_picks+12]
        
        def calc_display_score(u: InformationUnit) -> float:
            """è®¡ç®—æ˜¾ç¤ºåˆ†æ•° (1-10 scale)"""
            base = (u.analysis_depth_score * 0.6 + u.importance_score * 0.4) * 10
            # æ·»åŠ ä¸€äº›æ–¹å·®
            return round(min(9.8, max(6.5, base)), 1)
        
        def generate_reasoning(u: InformationUnit) -> str:
            """ç”Ÿæˆå…¥é€‰ç†ç”±"""
            if u.importance_score > 0.8:
                return "é‡è¦æ€§é«˜ï¼Œå€¼å¾—å…³æ³¨"
            elif u.analysis_depth_score > 0.8:
                return "åˆ†ææ·±åº¦è¾ƒå¥½"
            else:
                return "ç»¼åˆè¯„åˆ†å…¥é€‰"
        
        return {
            "daily_summary": "ä»Šæ—¥è‡ªåŠ¨ç®€æŠ¥ï¼ˆAIåˆ†æä¸´æ—¶ä¸å¯ç”¨ï¼‰",
            "top_picks": [
                {
                    "id": u.id,
                    "score": calc_display_score(u),
                    "display_title": u.title,
                    "reasoning": generate_reasoning(u),
                    "presentation": {
                        "summary": u.summary or "æš‚æ— æ‘˜è¦",
                        "analysis": u.analysis_content or "æš‚æ— æ·±åº¦åˆ†æ",
                        "impact": u.impact_assessment or "æš‚æ— å½±å“è¯„ä¼°"
                    }
                } for u in top
            ],
            "quick_reads": [
                {
                    "id": u.id,
                    "display_title": u.title,
                    "one_line_summary": u.summary[:50] if u.summary else u.title
                } for u in rest
            ],
            "excluded_reasons": {}
        }
