"""Information Extractor Agent - ä¿¡æ¯æå–ä¸“å®¶"""

import json
import hashlib
from typing import List
import structlog

from .base import BaseAgent
from ..models.article import Article
from ..models.information import InformationUnit, InformationType, SourceReference
from ..models.agent import AgentContext, AgentOutput
from ..models.analysis import Entity

logger = structlog.get_logger()

EXTRACTOR_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æƒ…æŠ¥æå–ä¸åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†è¾“å…¥çš„æ–°é—»æ–‡ç« æ‹†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„ã€é«˜ä»·å€¼çš„"ä¿¡æ¯å•å…ƒ" (Information Units)ã€‚

## ä»€ä¹ˆæ˜¯ä¿¡æ¯å•å…ƒï¼Ÿ
ä¿¡æ¯å•å…ƒæ˜¯å‘é€ç»™ç”¨æˆ·çš„æœ€å°æ•°æ®å•ä½ã€‚å®ƒåº”å½“æ˜¯åŸå­çš„ã€ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”åŒ…å«æ·±åº¦åˆ†æã€‚

## ä½ çš„èŒè´£
1. **åŸå­æ‹†åˆ†**ï¼šè¯†åˆ«æ–‡ç« ä¸­åŒ…å«çš„ç‹¬ç«‹äº‹å®ã€äº‹ä»¶æˆ–è§‚ç‚¹ã€‚å¦‚æœä¸€ç¯‡æ–‡ç« æŠ¥é“äº†ä¸¤ä¸ªä¸åŒè¯é¢˜ï¼Œè¯·å°†å…¶æ‹†åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ä¿¡æ¯å•å…ƒã€‚
2. **æ—¶é—´æ ‡æ³¨**ï¼šæ˜ç¡®æ ‡æ³¨äº‹ä»¶å‘ç”Ÿæ—¶é—´å’ŒæŠ¥é“æ—¶é—´ã€‚
3. **æ·±åº¦æå–**ï¼šä¸ä»…ä»…æå–è¡¨é¢äº‹å®ï¼Œæ›´è¦æå–èƒŒæ™¯ã€å½±å“å’Œæ·±å±‚å«ä¹‰ã€‚
4. **ä»·å€¼è¯„ä¼°**ï¼šå¯¹æ¯æ¡ä¿¡æ¯è¿›è¡Œ4ç»´ä»·å€¼è¯„ä¼°ã€‚

## è¾“å‡ºè¦æ±‚
è¯·è¾“å‡ºä¸€ä¸ª JSON åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

### åŸºç¡€å­—æ®µ
- `type`: fact(äº‹å®), opinion(è§‚ç‚¹), event(äº‹ä»¶), data(æ•°æ®)
- `title`: ç®€ç»ƒçš„æ ‡é¢˜ï¼ˆ20å­—ä»¥å†…ï¼‰
- `content`: è¯¦ç»†çš„å†…å®¹æè¿°ï¼ˆåŒ…å«äº‹å®ç»è¿‡ã€èƒŒæ™¯ä¿¡æ¯ï¼Œ200å­—å·¦å³ï¼‰
- `summary`: ä¸€å¥è¯æ ¸å¿ƒæ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰

### â° æ—¶é—´å­—æ®µï¼ˆé‡è¦ï¼ï¼‰
- `event_time`: äº‹ä»¶å‘ç”Ÿæ—¶é—´ï¼ˆå¦‚"2026å¹´1æœˆ15æ—¥"ï¼Œ"æœ¬å‘¨ä¸€"ï¼Œæˆ–"æœªæ˜ç¡®"ï¼‰
- `time_sensitivity`: urgent(ç´§æ€¥ï¼Œ24hå†…éœ€å…³æ³¨) / normal(æ­£å¸¸) / evergreen(å¸¸é’ï¼Œæ— æ—¶æ•ˆæ€§)

### ğŸ“Š 4ç»´ä»·å€¼è¯„ä¼°ï¼ˆ1-10åˆ†ï¼‰
- `information_gain`: ä¿¡æ¯å¢é‡ã€‚æ˜¯å¦æ‰“ç ´å·²çŸ¥å…±è¯†ï¼Ÿ(10=é¢ è¦†æ€§ï¼Œ5=ç¬¦åˆé¢„æœŸï¼Œ2=åºŸè¯å¤è¿°)
- `actionability`: è¡ŒåŠ¨æŒ‡å¯¼æ€§ã€‚èƒ½å¦æŒ‡å¯¼å…·ä½“å†³ç­–ï¼Ÿ(10=æ˜ç¡®å‚æ•°/æˆªæ­¢æ—¥æœŸï¼Œ5=æœ‰å‚è€ƒä»·å€¼ï¼Œ2=çº¯æƒ…ç»ª)
- `scarcity`: ç¨€ç¼ºæ€§ã€‚æ˜¯å¦ä¸€æ‰‹ä¿¡æºï¼Ÿ(10=åŸå§‹æ•°æ®/å®˜æ–¹å…¬å‘Šï¼Œ5=æƒå¨å¼•ç”¨ï¼Œ2=è‡ªåª’ä½“è½¬è¿°)
- `impact_magnitude`: å½±å“èŒƒå›´ã€‚æ¶‰åŠå®ä½“æƒé‡ï¼Ÿ(10=å¤®è¡Œ/è‹¹æœ/OpenAIï¼Œ5=è¡Œä¸šé¾™å¤´ï¼Œ2=è¾¹ç¼˜ç©å®¶)

### åˆ†æå­—æ®µ
- `analysis_content`: **åˆ†ææ¿å—**ï¼ˆéå¸¸é‡è¦ï¼ï¼‰ã€‚åŒ…å«æ·±åº¦è§£è¯»ã€è¶‹åŠ¿é¢„æµ‹ã€çŸ›ç›¾ç‚¹åˆ†æã€‚
- `key_insights`: [å…³é”®æ´å¯Ÿ1, å…³é”®æ´å¯Ÿ2...] (3-5ä¸ªæ·±åº¦è§‚ç‚¹)
- `analysis_depth_score`: 0.0-1.0 (åˆ†ææ·±åº¦è¯„åˆ†)

### 5W1H ç»“æ„åŒ–
- `who`: [æ¶‰åŠäººç‰©/ç»„ç»‡...]
- `what`: å‘ç”Ÿäº†ä»€ä¹ˆ
- `when`: æ—¶é—´
- `where`: åœ°ç‚¹
- `why`: åŸå› 
- `how`: æ–¹å¼/è¿‡ç¨‹

### å…ƒæ•°æ®
- `extraction_confidence`: 0.0-1.0 (æå–ç½®ä¿¡åº¦)
- `credibility_score`: 0.0-1.0 (å†…å®¹å¯ä¿¡åº¦)
- `importance_score`: 0.0-1.0 (é‡è¦æ€§ï¼ŒåŸºäº4ç»´è¯„ä¼°ç»¼åˆ)
- `sentiment`: positive/neutral/negative
- `impact_assessment`: ç®€è¿°å¯¹è¡Œä¸š/å¸‚åœº/ç¤¾ä¼šçš„æ½œåœ¨å½±å“
- `entities`: [{"name": "å®ä½“å", "type": "ç±»å‹", "description": "æè¿°"}, ...]
- `tags`: [æ ‡ç­¾1, æ ‡ç­¾2...]

## è¯„åˆ†ç¤ºä¾‹
**é«˜åˆ†ç¤ºä¾‹ (ç»¼åˆ 8+)**ï¼š
> "è‹±ä¼Ÿè¾¾å‘å¸ƒRTX 5090ï¼Œæ€§èƒ½æå‡70%ï¼Œå”®ä»·1999ç¾å…ƒï¼Œ2æœˆä¸Šå¸‚"
- information_gain=8 (æ˜¾è‘—æ€§èƒ½è·ƒå‡)
- actionability=9 (æ˜ç¡®æ—¶é—´ã€ä»·æ ¼)
- scarcity=9 (å®˜æ–¹å‘å¸ƒä¼š)
- impact_magnitude=9 (è‹±ä¼Ÿè¾¾æ˜¯æ ¸å¿ƒç©å®¶)

**ä½åˆ†ç¤ºä¾‹ (ç»¼åˆ 3)**ï¼š
> "ä¸“å®¶è®¤ä¸ºAIå°†æ”¹å˜ä¸–ç•Œ"
- information_gain=2 (è€ç”Ÿå¸¸è°ˆ)
- actionability=1 (æ— å…·ä½“ä¿¡æ¯)
- scarcity=2 (æ³›æ³›è€Œè°ˆ)
- impact_magnitude=4 (æ— å…·ä½“ä¸»ä½“)

## æ³¨æ„äº‹é¡¹
- å°½é‡ä¿ç•™åŸæ–‡çš„å…³é”®ç»†èŠ‚å’Œæ•°æ®ã€‚
- åˆ†æå†…å®¹ (`analysis_content`) å¿…é¡»æœ‰å®è´¨æ€§ï¼Œé¿å…åºŸè¯ã€‚
- å¦‚æœæ˜¯è®ºå›å¸–å­ã€æŠ€æœ¯é—®ç­”ã€ä¸ªäººæ±‚åŠ©ç­‰éæ–°é—»å†…å®¹ï¼Œ4ç»´è¯„åˆ†å‡åº”åä½ï¼ˆâ‰¤4ï¼‰ã€‚
"""

class InformationExtractorAgent(BaseAgent):
    """
    ä¿¡æ¯æå– Agent
    
    èŒè´£ï¼š
    1. å°† Article æ‹†è§£ä¸º List[InformationUnit]
    2. ç”Ÿæˆåˆæ­¥çš„æ·±åº¦åˆ†æ
    """
    
    AGENT_NAME = "Extractor"
    SYSTEM_PROMPT = EXTRACTOR_SYSTEM_PROMPT
    
    async def process(self, input_data: Article, context: AgentContext) -> AgentOutput:
        """æ‰§è¡Œæå–ä»»åŠ¡"""
        article = input_data
        self.log_start(article.title)
        
        user_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼Œæå–ä¿¡æ¯å•å…ƒï¼š

        æ ‡é¢˜: {article.title}
        æ¥æº: {article.source}
        å‘å¸ƒæ—¶é—´: {article.published_at}
        å†…å®¹:
        {article.content}
        """
        
        result, token_usage = await self.invoke_llm(
            user_prompt=user_prompt,
            max_tokens=4000,
            temperature=0.3,
            json_mode=True
        )
        
        units = []
        if result and isinstance(result, list):
            for item in result:
                try:
                    unit = self._parse_unit(item, article)
                    units.append(unit)
                except Exception as e:
                    logger.error("unit_parsing_failed", error=str(e), item_summary=str(item)[:100])
        
        trace = self.create_trace(
            input_summary=f"Article: {article.title}",
            output_summary=f"Extracted {len(units)} units",
            duration=0, 
            token_usage=token_usage
        )
        
        self.log_complete(0, f"Units: {len(units)}")
        return AgentOutput(success=True, data=units, trace=trace)
        
    async def extract(self, article: Article, context: AgentContext) -> List[InformationUnit]:
        """Deprecated alias for process"""
        output = await self.process(article, context)
        return output.data

    def _parse_unit(self, item: dict, article: Article) -> InformationUnit:
        """è§£æ LLM è¿”å›çš„ JSON ä¸º InformationUnit å¯¹è±¡"""
        
        # ç”ŸæˆæŒ‡çº¹ (Content Based Hash)
        content_str = f"{item.get('title', '')}{item.get('content', '')}"
        fingerprint = hashlib.md5(content_str.encode()).hexdigest()
        
        # ç”Ÿæˆ ID
        unit_id = f"iu_{fingerprint[:16]}"
        
        # å¤„ç†å®ä½“
        entities = []
        for e in item.get("entities", []):
            if isinstance(e, dict):
                entities.append(Entity(
                    name=e.get("name", ""),
                    type=e.get("type", "unknown"),
                    description=e.get("description", "")
                ))
        
        # æ„å»ºæ¥æºå¼•ç”¨
        source_ref = SourceReference(
            url=article.url,
            title=article.title,
            source_name=article.source,
            published_at=article.published_at,
            excerpt=article.summary[:200] if article.summary else "",
            credibility_tier="unknown"
        )
        
        # å¤„ç†4ç»´è¯„åˆ†ï¼ˆLLMè¿”å›1-10ï¼Œç»Ÿä¸€è½¬æ¢ï¼‰
        def safe_score(val, default=5.0, scale=1):
            """å®‰å…¨è·å–è¯„åˆ†ï¼Œç¡®ä¿åœ¨1-10èŒƒå›´å†…"""
            try:
                score = float(val) if val else default
                return max(1.0, min(10.0, score * scale))
            except:
                return default
        
        return InformationUnit(
            id=unit_id,
            fingerprint=fingerprint,
            type=InformationType(item.get("type", "fact")),
            title=item.get("title", "") or article.title,
            content=item.get("content", "") or article.content,
            summary=item.get("summary", ""),
            
            # æ—¶é—´å­—æ®µ
            event_time=item.get("event_time") or item.get("when") or None,
            report_time=article.published_at,
            time_sensitivity=item.get("time_sensitivity", "normal"),
            
            # åˆ†æå­—æ®µ
            analysis_content=item.get("analysis_content", ""),
            key_insights=item.get("key_insights", []),
            analysis_depth_score=float(item.get("analysis_depth_score", 0.5)),
            
            # 4ç»´ä»·å€¼è¯„åˆ†
            information_gain=safe_score(item.get("information_gain"), 5.0),
            actionability=safe_score(item.get("actionability"), 5.0),
            scarcity=safe_score(item.get("scarcity"), 5.0),
            impact_magnitude=safe_score(item.get("impact_magnitude"), 5.0),
            
            # 5W1H
            who=item.get("who", []),
            what=item.get("what", ""),
            when=item.get("when", ""),
            where=item.get("where", ""),
            why=item.get("why", ""),
            how=item.get("how", ""),
            
            # æ¥æº
            primary_source=article.url,
            extraction_confidence=float(item.get("extraction_confidence", 0.8)),
            
            # åˆ†æç»“æœ
            credibility_score=float(item.get("credibility_score", 0.5)),
            importance_score=float(item.get("importance_score", 0.5)),
            sentiment=item.get("sentiment", "neutral"),
            impact_assessment=item.get("impact_assessment", ""),
            
            # å…ƒæ•°æ®
            entities=entities,
            tags=item.get("tags", []),
            created_at=article.fetched_at,
            sources=[source_ref]
        )

