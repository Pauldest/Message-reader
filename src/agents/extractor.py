"""Information Extractor Agent - ä¿¡æ¯æå–ä¸“å®¶"""

import json
import hashlib
from typing import List
import structlog

from .base import BaseAgent
from ..models.article import Article
from ..models.information import InformationUnit, InformationType, SourceReference
from ..models.agent import AgentContext, AgentOutput
from ..models.analysis import Entity

logger = structlog.get_logger()

EXTRACTOR_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æƒ…æŠ¥æå–ä¸åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†è¾“å…¥çš„æ–°é—»æ–‡ç« æ‹†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„ã€é«˜ä»·å€¼çš„"ä¿¡æ¯å•å…ƒ" (Information Units)ã€‚

## ä»€ä¹ˆæ˜¯ä¿¡æ¯å•å…ƒï¼Ÿ
ä¿¡æ¯å•å…ƒæ˜¯å‘é€ç»™ç”¨æˆ·çš„æœ€å°æ•°æ®å•ä½ã€‚å®ƒåº”å½“æ˜¯åŸå­çš„ã€ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”åŒ…å«æ·±åº¦åˆ†æã€‚

## ä½ çš„èŒè´£
1. **åŸå­æ‹†åˆ†**ï¼šè¯†åˆ«æ–‡ç« ä¸­åŒ…å«çš„ç‹¬ç«‹äº‹å®ã€äº‹ä»¶æˆ–è§‚ç‚¹
2. **æ—¶é—´æ ‡æ³¨**ï¼šæ˜ç¡®æ ‡æ³¨äº‹ä»¶å‘ç”Ÿæ—¶é—´
3. **çŠ¶æ€åˆ†ç±»**ï¼šè¯†åˆ«è¿™æ˜¯å“ªç§ç±»å‹çš„çŠ¶æ€æ”¹å˜
4. **å®ä½“é”šå®š**ï¼šå°†å®ä½“å½’ç±»åˆ°é¢„è®¾çš„æ¯å®ä½“ä¸‹
5. **ä»·å€¼è¯„ä¼°**ï¼šè¿›è¡Œ4ç»´ä»·å€¼è¯„ä¼°

## è¾“å‡ºè¦æ±‚ (JSON åˆ—è¡¨)

### åŸºç¡€å­—æ®µ
- `type`: fact(äº‹å®), opinion(è§‚ç‚¹), event(äº‹ä»¶), data(æ•°æ®)
- `title`: ç®€ç»ƒæ ‡é¢˜ï¼ˆ20å­—ä»¥å†…ï¼‰
- `content`: è¯¦ç»†å†…å®¹ï¼ˆ200å­—å·¦å³ï¼‰
- `summary`: ä¸€å¥è¯æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰

### â° æ—¶é—´å­—æ®µ
- `event_time`: äº‹ä»¶å‘ç”Ÿæ—¶é—´ï¼ˆå¦‚"2026å¹´1æœˆ15æ—¥"ï¼‰
- `time_sensitivity`: urgent / normal / evergreen

### ğŸ“Š 4ç»´ä»·å€¼è¯„ä¼°ï¼ˆ1-10åˆ†ï¼‰
- `information_gain`: ä¿¡æ¯å¢é‡ï¼ˆ10=é¢ è¦†å…±è¯†ï¼Œ5=ç¬¦åˆé¢„æœŸï¼Œ2=åºŸè¯ï¼‰
- `actionability`: è¡ŒåŠ¨æŒ‡å¯¼ï¼ˆ10=æ˜ç¡®å‚æ•°ï¼Œ5=æœ‰å‚è€ƒï¼Œ2=çº¯æƒ…ç»ªï¼‰
- `scarcity`: ç¨€ç¼ºæ€§ï¼ˆ10=ä¸€æ‰‹ä¿¡æºï¼Œ5=æƒå¨å¼•ç”¨ï¼Œ2=è½¬è¿°ï¼‰
- `impact_magnitude`: å½±å“èŒƒå›´ï¼ˆ10=æ ¸å¿ƒç©å®¶ï¼Œ5=è¡Œä¸šé¾™å¤´ï¼Œ2=è¾¹ç¼˜ï¼‰

### ğŸ·ï¸ HEX çŠ¶æ€åˆ†ç±»ï¼ˆå¿…å¡«ï¼ï¼‰
ä»ä»¥ä¸‹ 6 ç±»ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ **1 ä¸ª**ä½œä¸º `state_change_type`:
- `TECH`: ğŸ§¬ æŠ€æœ¯/äº§å“å˜åŒ–ï¼ˆå‘å¸ƒã€è¿­ä»£ã€çªç ´ã€ä¸“åˆ©ï¼‰
- `CAPITAL`: ğŸ’° èµ„æœ¬/å¸‚åœºå˜åŒ–ï¼ˆèèµ„ã€è´¢æŠ¥ã€å¹¶è´­ã€è‚¡ä»·ï¼‰
- `REGULATION`: âš–ï¸ è§„åˆ™/æ”¿ç­–å˜åŒ–ï¼ˆæ³•è§„ã€åˆ¶è£ã€åå„æ–­ã€åˆè§„ï¼‰
- `ORG`: ğŸ‘” ç»„ç»‡/äººäº‹å˜åŒ–ï¼ˆé«˜ç®¡ã€è£å‘˜ã€æ¶æ„è°ƒæ•´ï¼‰
- `RISK`: âš ï¸ é£é™©/å±æœºäº‹ä»¶ï¼ˆé»‘å®¢ã€å®•æœºã€ä¸‘é—»ã€äº‹æ•…ï¼‰
- `SENTIMENT`: ğŸ—£ï¸ å…±è¯†/æƒ…ç»ªå˜åŒ–ï¼ˆè¯„çº§ã€èˆ†è®ºåè½¬ã€å…³é”®è¡¨æ€ï¼‰

åŒæ—¶æä¾› `state_change_subtypes` åˆ—è¡¨ï¼ˆå¦‚ ["äº§å“å‘å¸ƒ", "ç‰ˆæœ¬è¿­ä»£"]ï¼‰

### ğŸ¯ ä¸‰çº§å®ä½“é”šå®šï¼ˆå¿…å¡«ï¼ï¼‰
ä¸ºæ–°é—»ä¸­çš„**ä¸»è§’å®ä½“**ç”Ÿæˆä¸‰çº§æ ‡ç­¾ï¼Œè¾“å‡ºåˆ° `entity_hierarchy` åˆ—è¡¨ï¼š

**L3 æ¯å®ä½“ (l3_root)** - å¿…é¡»ä»ä»¥ä¸‹é¢„è®¾åˆ—è¡¨é€‰æ‹©:
äººå·¥æ™ºèƒ½, åŠå¯¼ä½“èŠ¯ç‰‡, æ¶ˆè´¹ç”µå­, äº‘è®¡ç®—ä¸æ•°æ®ä¸­å¿ƒ, è½¯ä»¶ä¸å¼€å‘å·¥å…·, åŒºå—é“¾ä¸åŠ å¯†è´§å¸, ç½‘ç»œå®‰å…¨, ç”µå•†ä¸é›¶å”®, ç¤¾äº¤åª’ä½“, æ¸¸æˆä¸å¨±ä¹, å†…å®¹ä¸æµåª’ä½“, é‡‘èä¸é“¶è¡Œ, æ±½è½¦ä¸å‡ºè¡Œ, èƒ½æºä¸ç¯å¢ƒ, åŒ»ç–—ä¸ç”Ÿç‰©ç§‘æŠ€, åˆ¶é€ ä¸å·¥ä¸š, å®è§‚ç»æµ, åœ°ç¼˜æ”¿æ²»

**L2 ç»†åˆ†é¢†åŸŸ (l2_sector)** - è‡ªç”±ç”Ÿæˆï¼Œæè¿°å®ä½“åœ¨è¡Œä¸šä¸­çš„ä½ç½®ï¼ˆå¦‚"åŸºç¡€æ¨¡å‹"ã€"AIèŠ¯ç‰‡"ï¼‰

**L1 å¶å­å®ä½“ (l1_name)** - æ–°é—»ä¸­å‡ºç°çš„å…·ä½“åç§°

**æ³¨æ„**ï¼šå¦‚æœä¸€ä¸ªå®ä½“è·¨è¶Šå¤šä¸ªæ¯å®ä½“ï¼ˆå¦‚ NVIDIA æ—¢æ˜¯AIä¹Ÿæ˜¯åŠå¯¼ä½“ï¼‰ï¼Œç”Ÿæˆå¤šæ¡è®°å½•ã€‚

### åˆ†æå­—æ®µ
- `analysis_content`: æ·±åº¦è§£è¯»ï¼ˆ100-200å­—ï¼‰
- `key_insights`: [æ´å¯Ÿ1, æ´å¯Ÿ2...] (3-5ä¸ª)
- `analysis_depth_score`: 0.0-1.0

### 5W1H ç»“æ„åŒ–
- `who`, `what`, `when`, `where`, `why`, `how`

### å…ƒæ•°æ®
- `extraction_confidence`, `credibility_score`, `importance_score`
- `sentiment`, `impact_assessment`, `entities`, `tags`

## è¾“å‡ºç¤ºä¾‹
```json
{
  "type": "event",
  "title": "è‹±ä¼Ÿè¾¾å‘å¸ƒRTX5090",
  "state_change_type": "TECH",
  "state_change_subtypes": ["äº§å“å‘å¸ƒ"],
  "entity_hierarchy": [
    {"l1_name": "NVIDIA", "l1_role": "ä¸»è§’", "l2_sector": "æ¶ˆè´¹çº§GPU", "l3_root": "åŠå¯¼ä½“èŠ¯ç‰‡"},
    {"l1_name": "NVIDIA", "l1_role": "ä¸»è§’", "l2_sector": "AIèŠ¯ç‰‡", "l3_root": "äººå·¥æ™ºèƒ½"}
  ],
  "event_time": "2026å¹´1æœˆ17æ—¥",
  "information_gain": 8,
  "actionability": 9,
  "scarcity": 9,
  "impact_magnitude": 9
}
```

## æ³¨æ„äº‹é¡¹
- è®ºå›å¸–å­ã€æŠ€æœ¯é—®ç­”ç­‰éæ–°é—»å†…å®¹ï¼Œ4ç»´è¯„åˆ†åº”åä½ï¼ˆâ‰¤4ï¼‰
- åˆ†æå†…å®¹å¿…é¡»æœ‰å®è´¨æ€§ï¼Œé¿å…åºŸè¯
"""

class InformationExtractorAgent(BaseAgent):
    """
    ä¿¡æ¯æå– Agent
    
    èŒè´£ï¼š
    1. å°† Article æ‹†è§£ä¸º List[InformationUnit]
    2. ç”Ÿæˆåˆæ­¥çš„æ·±åº¦åˆ†æ
    """
    
    AGENT_NAME = "Extractor"
    SYSTEM_PROMPT = EXTRACTOR_SYSTEM_PROMPT
    
    async def process(self, input_data: Article, context: AgentContext) -> AgentOutput:
        """æ‰§è¡Œæå–ä»»åŠ¡"""
        article = input_data
        self.log_start(article.title)
        
        user_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼Œæå–ä¿¡æ¯å•å…ƒï¼š

        æ ‡é¢˜: {article.title}
        æ¥æº: {article.source}
        å‘å¸ƒæ—¶é—´: {article.published_at}
        å†…å®¹:
        {article.content}
        """
        
        result, token_usage = await self.invoke_llm(
            user_prompt=user_prompt,
            max_tokens=4000,
            temperature=0.3,
            json_mode=True
        )
        
        units = []
        if result and isinstance(result, list):
            for item in result:
                try:
                    unit = self._parse_unit(item, article)
                    units.append(unit)
                except Exception as e:
                    logger.error("unit_parsing_failed", error=str(e), item_summary=str(item)[:100])
        
        trace = self.create_trace(
            input_summary=f"Article: {article.title}",
            output_summary=f"Extracted {len(units)} units",
            duration=0, 
            token_usage=token_usage
        )
        
        self.log_complete(0, f"Units: {len(units)}")
        return AgentOutput(success=True, data=units, trace=trace)
        
    async def extract(self, article: Article, context: AgentContext) -> List[InformationUnit]:
        """Deprecated alias for process"""
        output = await self.process(article, context)
        return output.data

    def _parse_unit(self, item: dict, article: Article) -> InformationUnit:
        """è§£æ LLM è¿”å›çš„ JSON ä¸º InformationUnit å¯¹è±¡"""
        from ..models.information import EntityAnchor, ROOT_ENTITIES
        
        # ç”ŸæˆæŒ‡çº¹ (Content Based Hash)
        content_str = f"{item.get('title', '')}{item.get('content', '')}"
        fingerprint = hashlib.md5(content_str.encode()).hexdigest()
        
        # ç”Ÿæˆ ID
        unit_id = f"iu_{fingerprint[:16]}"
        
        # å¤„ç†å®ä½“
        entities = []
        for e in item.get("entities", []):
            if isinstance(e, dict):
                entities.append(Entity(
                    name=e.get("name", ""),
                    type=e.get("type", "unknown"),
                    description=e.get("description", "")
                ))
        
        # å¤„ç†å®ä½“é”šå®š
        entity_hierarchy = []
        for eh in item.get("entity_hierarchy", []):
            if isinstance(eh, dict):
                l3_root = eh.get("l3_root", "")
                # éªŒè¯ L3 æ˜¯å¦åœ¨é¢„è®¾åˆ—è¡¨ä¸­
                if l3_root and l3_root not in ROOT_ENTITIES:
                    # å°è¯•æ¨¡ç³ŠåŒ¹é…
                    for root in ROOT_ENTITIES:
                        if l3_root in root or root in l3_root:
                            l3_root = root
                            break
                    else:
                        l3_root = "å…¶ä»–"  # é»˜è®¤å½’ç±»
                
                entity_hierarchy.append(EntityAnchor(
                    l1_name=eh.get("l1_name", ""),
                    l1_role=eh.get("l1_role", "ä¸»è§’"),
                    l2_sector=eh.get("l2_sector", ""),
                    l3_root=l3_root,
                    confidence=float(eh.get("confidence", 0.8))
                ))
        
        # æ„å»ºæ¥æºå¼•ç”¨
        source_ref = SourceReference(
            url=article.url,
            title=article.title,
            source_name=article.source,
            published_at=article.published_at,
            excerpt=article.summary[:200] if article.summary else "",
            credibility_tier="unknown"
        )
        
        # å¤„ç†4ç»´è¯„åˆ†
        def safe_score(val, default=5.0):
            try:
                score = float(val) if val else default
                return max(1.0, min(10.0, score))
            except:
                return default
        
        # éªŒè¯ HEX çŠ¶æ€ç±»å‹
        state_type = item.get("state_change_type", "")
        valid_types = ["TECH", "CAPITAL", "REGULATION", "ORG", "RISK", "SENTIMENT"]
        if state_type not in valid_types:
            state_type = ""
        
        return InformationUnit(
            id=unit_id,
            fingerprint=fingerprint,
            type=InformationType(item.get("type", "fact")),
            title=item.get("title", "") or article.title,
            content=item.get("content", "") or article.content,
            summary=item.get("summary", ""),
            
            # æ—¶é—´å­—æ®µ
            event_time=item.get("event_time") or item.get("when") or None,
            report_time=article.published_at,
            time_sensitivity=item.get("time_sensitivity", "normal"),
            
            # åˆ†æå­—æ®µ
            analysis_content=item.get("analysis_content", ""),
            key_insights=item.get("key_insights", []),
            analysis_depth_score=float(item.get("analysis_depth_score", 0.5)),
            
            # 4ç»´ä»·å€¼è¯„åˆ†
            information_gain=safe_score(item.get("information_gain"), 5.0),
            actionability=safe_score(item.get("actionability"), 5.0),
            scarcity=safe_score(item.get("scarcity"), 5.0),
            impact_magnitude=safe_score(item.get("impact_magnitude"), 5.0),
            
            # HEX çŠ¶æ€åˆ†ç±»
            state_change_type=state_type,
            state_change_subtypes=item.get("state_change_subtypes", []) if isinstance(item.get("state_change_subtypes"), list) else [],
            
            # ä¸‰çº§å®ä½“é”šå®š
            entity_hierarchy=entity_hierarchy,
            
            # 5W1H (å¤„ç†å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨çš„æƒ…å†µ)
            who=item.get("who", []) if isinstance(item.get("who"), list) else [item.get("who")] if item.get("who") else [],
            what=item.get("what", "") if isinstance(item.get("what"), str) else str(item.get("what", "")),
            when=item.get("when", "") if isinstance(item.get("when"), str) else str(item.get("when", "")),
            where=item.get("where", "") if isinstance(item.get("where"), str) else str(item.get("where", "")),
            why=item.get("why", "") if isinstance(item.get("why"), str) else str(item.get("why", "")),
            how=item.get("how", "") if isinstance(item.get("how"), str) else str(item.get("how", "")),
            
            # æ¥æº
            primary_source=article.url,
            extraction_confidence=float(item.get("extraction_confidence", 0.8)),
            
            # åˆ†æç»“æœ
            credibility_score=float(item.get("credibility_score", 0.5)),
            importance_score=float(item.get("importance_score", 0.5)),
            sentiment=item.get("sentiment", "neutral"),
            impact_assessment=item.get("impact_assessment", ""),
            
            # å…ƒæ•°æ®
            entities=entities,
            tags=item.get("tags", []),
            created_at=article.fetched_at,
            sources=[source_ref]
        )

